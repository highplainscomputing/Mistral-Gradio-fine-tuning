{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/highplainscomputing/Mistral-Gradio-fine-tuning/blob/main/RAG_Demo_Restricted.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voZv9isaAExl"
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3oJyVq0LcRx"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/highplainscomputing/Mistral-Gradio-fine-tuning.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r /content/Mistral-Gradio-fine-tuning/requirements.txt\n",
        "# !pip install -q nemoguardrails==0.8.1"
      ],
      "metadata": {
        "id": "t22EFzRsfuJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JKaWOGQ2I6k"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km2ornvJ9ft5"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjxabIdYLdN0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
        "from io import StringIO\n",
        "from datasets import load_dataset, Dataset\n",
        "from langchain.prompts import PromptTemplate, MessagesPlaceholder, ChatPromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain import HuggingFaceHub\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "import os\n",
        "import gradio as gr\n",
        "from operator import itemgetter\n",
        "import shutil\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from threading import Thread\n",
        "from transformers.utils import logging\n",
        "logging.get_logger(\"transformers\").setLevel(logging.ERROR)\n",
        "import langchain_core\n",
        "# nemo Guardrails\n",
        "\n",
        "from nemoguardrails import RailsConfig, LLMRails\n",
        "from nemoguardrails.integrations.langchain.runnable_rails import RunnableRails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XtOACw0PSD2"
      },
      "source": [
        "# Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxNpWqqiQRUP"
      },
      "outputs": [],
      "source": [
        "LOAD_IN_4BIT = True\n",
        "BNB_4BIT_USE_DOUBLE_QUANT = True\n",
        "BNB_4BIT_QUANT_TYPE = \"nf4\"\n",
        "ADD_EOS_TOKEN = True\n",
        "PADDING_SIDE = \"left\"\n",
        "ADD_BOS_TOKEN = True\n",
        "HUGGINGFACE_API_KEY = \"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\"\n",
        "FIX_RESPONSE = \"I am a Mental health assistant. I don't answer non medical health related question.\"\n",
        "RAG_COLANG_CONTENT = \"\"\"\n",
        "# define limits\n",
        "define user ask doctor\n",
        "    \"Can you refer me to a doctor?\"\n",
        "\n",
        "\n",
        "define bot answer doctor\n",
        "    \"I'm a Medical health assistant\"\n",
        "    \"Sorry I can't recommend a doctor!\"\n",
        "\n",
        "define flow doctor\n",
        "    user ask doctor\n",
        "    bot answer doctor\n",
        "    bot offer help\n",
        "#\n",
        "define user ask general\n",
        "    \"what is Artifical Intelligence?\"\n",
        "    \"what are Transformer models?\"\n",
        "\n",
        "define bot answer general\n",
        "    \"I'm a Medical health assistant, I can only help you out with medical health issues.\"\n",
        "    \"Sorry I can't answer that!\"\n",
        "\n",
        "define flow medicine\n",
        "    user ask general\n",
        "    bot answer general\n",
        "    bot offer help\"\n",
        "\n",
        "# Basic guardrail against insults.\n",
        "define flow\n",
        "  user express insult\n",
        "  bot express calmly willingness to help\n",
        "\n",
        "# Basic guardrail against non medical question.\n",
        "define flow\n",
        "  user express non medical health question\n",
        "  bot express apology willingness to help\n",
        "\n",
        "# define RAG intents and flow\n",
        "# Here we use the QA chain for anything else.\n",
        "define flow\n",
        "  user ...\n",
        "  $contexts = execute retrieve(query=$last_user_message)\n",
        "  $answer = execute qa_chain(query=$last_user_message, contexts=$contexts)\n",
        "  bot $answer\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  os.mkdir(\"data\")\n",
        "except:\n",
        "  print(\"Already Exists.\")"
      ],
      "metadata": {
        "id": "YtbtiA19175p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlyEzEOWPQJs"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uje-qnkKLpnn"
      },
      "outputs": [],
      "source": [
        "class RAG_Parameters:\n",
        "  def __init__(self, files = None, question_col_name = \"Context\", answer_col_name = \"Response\", model_name_choice = [\"mistralai/Mistral-7B-Instruct-v0.1\", \"meta-llama/Llama-2-7b-chat\"],\n",
        "               prompt_choices = [\"mistral_prompt\", \"llama_prompt\", \"restrict\"],\n",
        "               embedding_model = \"sentence-transformers/all-mpnet-base-v2\", temperature = 0.7, max_new_tokens = 128, repetition_penalty = 1.15,\n",
        "               top_k = 1, top_p = 0.75, k_context = 4, chunk_size = 512, chunk_overlap = 100):\n",
        "\n",
        "    # Dataset Parameters\n",
        "    self.files = files\n",
        "    self.question_col_name = question_col_name\n",
        "    self.answer_col_name = answer_col_name\n",
        "\n",
        "    # Model Parameters\n",
        "    self.model_name_choice = model_name_choice\n",
        "\n",
        "    # Config Parameters\n",
        "    self.prompt_choices = prompt_choices\n",
        "    self.embedding_model = embedding_model\n",
        "    self.temperature = temperature\n",
        "    self.max_new_tokens = max_new_tokens\n",
        "    self.repetition_penalty = repetition_penalty\n",
        "    self.top_k = top_k\n",
        "    self.top_p = top_p\n",
        "    self.k_context = k_context\n",
        "    self.chunk_size = chunk_size\n",
        "    self.chunk_overlap = chunk_overlap\n",
        "\n",
        "  # Functions to modify variables\n",
        "  def set_files(self, files):\n",
        "    self.files = files\n",
        "\n",
        "  def set_model_name_choice(self, model_name_choice):\n",
        "    self.model_name_choice = model_name_choice\n",
        "\n",
        "  def set_prompt_choices(self, prompt_choices):\n",
        "    self.prompt_choices = prompt_choices\n",
        "\n",
        "  def set_embedding_model(self, embedding_model):\n",
        "    self.embedding_model = embedding_model\n",
        "\n",
        "  def set_temperature(self, temperature):\n",
        "    self.temperature = temperature\n",
        "\n",
        "  def set_max_new_tokens(self, max_new_tokens):\n",
        "    self.max_new_tokens = max_new_tokens\n",
        "\n",
        "  def set_repetition_penalty(self, repetition_penalty):\n",
        "    self.repetition_penalty = repetition_penalty\n",
        "\n",
        "  def set_top_k(self, top_k):\n",
        "    self.top_k = top_k\n",
        "\n",
        "  def set_top_p(self, top_p):\n",
        "    self.top_p = top_p\n",
        "\n",
        "  def set_k_context(self, k_context):\n",
        "    self.k_context = k_context\n",
        "\n",
        "  def set_chunk_size(self, chunk_size):\n",
        "    self.chunk_size = chunk_size\n",
        "\n",
        "  def set_chunk_overlap(self, chunk_overlap):\n",
        "    self.chunk_overlap = chunk_overlap\n",
        "\n",
        "  def success_msg_params(self):\n",
        "    return \"Parameters set successfully\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSQ4YYeXLAzP"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNnhizwCKqoE"
      },
      "outputs": [],
      "source": [
        "class Data(RAG_Parameters):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.directory = \"/content/drive/MyDrive/MentalHealthFAQ/some_sample_leaflets\"\n",
        "\n",
        "  def loading_dataset(self):\n",
        "    if self.files != None:\n",
        "      print(f\"Files duplicates : {self.files}\")\n",
        "      self.files = list(set(self.files)) # remove duplicate path\n",
        "      print(f\"Files without duplicates : {self.files}\")\n",
        "      self.file_extension = self.files[0]\n",
        "      # print(f\"Files : {self.files}\")\n",
        "      # print(\"-------------------------------------------------------------------------------------------------------------\")\n",
        "      # print(f\"Type of files {type(self.files)}\")\n",
        "      for file_path in self.files:\n",
        "      # Extract the file name from the file path\n",
        "        file_name = os.path.basename(file_path)\n",
        "        # Construct the destination path\n",
        "        destination_path = os.path.join(self.directory, file_name)\n",
        "        # Move the file\n",
        "        shutil.move(file_path, destination_path)\n",
        "        print(f\"Moved {file_name} to {self.directory}\")\n",
        "    else:\n",
        "      print(\"-------------------------------------------------------------------------------------------------------------------\")\n",
        "      print(\"Please Upload Files in UI\")\n",
        "\n",
        "  def get_file_types(self):\n",
        "    file_types = set()\n",
        "    for filename in os.listdir(self.directory):\n",
        "      if os.path.isfile(os.path.join(self.directory, filename)):\n",
        "        file_extension = filename.split(\".\")[-1].lower()\n",
        "        file_types.add(file_extension)\n",
        "\n",
        "      total_size = os.path.getsize(self.directory)\n",
        "      self.size_of_files = total_size / 1024  # Size in KB\n",
        "      self.size_of_files = \"KB\".format(self.size_of_files)\n",
        "      self.file_types_string = \", \".join(file_types)\n",
        "      self.number_of_files = len(os.listdir(self.directory))\n",
        "\n",
        "    return self.file_types_string, self.number_of_files, self.size_of_files\n",
        "\n",
        "  def create_vector_store_index(self, file_extension):\n",
        "    file_extension, directory, _ = self.get_file_types()\n",
        "    print(f\"File Extension : {file_extension}\")\n",
        "    print(f\"Directory : {directory}\")\n",
        "    print(f\"Size : {_}\")\n",
        "    if file_extension == \"md\":\n",
        "      loader = DirectoryLoader(\n",
        "          self.directory,\n",
        "          glob=\"*.md\",\n",
        "          loader_cls=TextLoader,\n",
        "          show_progress=True,\n",
        "          )\n",
        "      pages = loader.load()\n",
        "      text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = self.chunk_size,\n",
        "      chunk_overlap = self.chunk_overlap,\n",
        "      )\n",
        "      self.documents = text_splitter.split_documents(pages)\n",
        "\n",
        "    elif file_extension == \"pdf\":\n",
        "      loader = DirectoryLoader(\n",
        "      self.directory,\n",
        "      glob=\"*.pdf\",\n",
        "      loader_cls=PyPDFLoader,\n",
        "      show_progress=True,\n",
        "      silent_errors=True,\n",
        "      )\n",
        "      pages = loader.load()\n",
        "      text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = self.chunk_size,\n",
        "      chunk_overlap = self.chunk_overlap,\n",
        "      )\n",
        "      self.documents = text_splitter.split_documents(pages)\n",
        "\n",
        "\n",
        "  def success_msg_data(self):\n",
        "    return \"Documents created successfully\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCSzcOaMK98c"
      },
      "source": [
        "# Create Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oo3THyQKzb8"
      },
      "outputs": [],
      "source": [
        "class Create_vector(Data):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def load_embedding_model(self):\n",
        "    try:\n",
        "      self.embeddings = HuggingFaceEmbeddings(model_name=\"model/embedding_model/\")\n",
        "    except:\n",
        "      self.embeddings = HuggingFaceEmbeddings(model_name=self.embedding_model)\n",
        "\n",
        "\n",
        "  def create_db(self):\n",
        "    self.vectordb = FAISS.from_documents(self.documents, self.embeddings)\n",
        "    # self.vectordb.save_local(\"vectorDB_for_mental_health\")\n",
        "    self.retriever_qa = self.vectordb.as_retriever(search_kwargs={\"k\": self.k_context})\n",
        "    self.retriever = self.vectordb.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.1, \"k\": self.k_context}) # set here to that all parameters can be set in UI\n",
        "\n",
        "  def load_db(self):\n",
        "    self.loadDB = FAISS.load_local(\"/content/drive/MyDrive/vectorDB_for_mental_health\", self.embeddings, allow_dangerous_deserialization=True)\n",
        "    num_documents_before = len(self.loadDB.index_to_docstore_id)\n",
        "    print(f\"Total number of documents before adding docs: {num_documents_before}\")\n",
        "    print(type(self.loadDB))\n",
        "    self.local_retriever_qa = self.loadDB.as_retriever(search_kwargs={\"k\": self.k_context})\n",
        "    self.local_retriever = self.loadDB.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.1, \"k\": self.k_context}) # set here to that all parameters can be set in UI\n",
        "\n",
        "\n",
        "  def reindex_db(self):\n",
        "    # add Docs\n",
        "    self.mergeDB = self.loadDB.add_documents(self.documents)\n",
        "    num_documents_after = len(self.loadDB.index_to_docstore_id)\n",
        "    print(f\"Total number of documents after adding docs: {num_documents_after}\")\n",
        "    self.local_retriever_qa = self.loadDB.as_retriever(search_kwargs={\"k\": self.k_context})\n",
        "    self.local_retriever = self.loadDB.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.1, \"k\": self.k_context}) # set here to that all parameters can be set in UI\n",
        "\n",
        "  def success_msg_data_db(self):\n",
        "    return \"Split data into chunks. Download embedding model, create vector database and retriever chain.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_JMi8tILECh"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRwCJHWDLpj1"
      },
      "outputs": [],
      "source": [
        "class Model(Create_vector):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.config = None\n",
        "\n",
        "\n",
        "  def set_config(self):\n",
        "    self.bnb_config = BitsAndBytesConfig(\n",
        "                                        load_in_4bit = LOAD_IN_4BIT,\n",
        "                                        bnb_4bit_use_double_quant = BNB_4BIT_USE_DOUBLE_QUANT,\n",
        "                                        bnb_4bit_quant_type = BNB_4BIT_QUANT_TYPE,\n",
        "                                        bnb_4bit_compute_dtype = torch.bfloat16\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def load_model(self):\n",
        "    # self.set_config()\n",
        "    if self.model_name_choice == \"mistralai/Mistral-7B-Instruct-v0.1\":\n",
        "      try:\n",
        "        print(\"-------------------------BEFORE LOADING MODEL LOCAL TRY----------------------------\")\n",
        "        self.load_base_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", quantization_config = self.bnb_config, low_cpu_mem_usage=True)\n",
        "        self.model = PeftModel.from_pretrained(self.load_base_model, \"/content/drive/MyDrive/mistral_instruct_r32a32/Mistral-7B-v0.1-Fine_tuned_model_dir/checkpoint-100\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "                                          \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "                                          padding_side=PADDING_SIDE,\n",
        "                                          add_eos_token=ADD_EOS_TOKEN,\n",
        "                                          add_bos_token=ADD_BOS_TOKEN,\n",
        "                                                  )\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        self.tokenizer = tokenizer\n",
        "        print(\"-------------------------AFTER LOADING MODEL LOCAL TRY----------------------------\")\n",
        "      except:\n",
        "        print(\"-------------------------BEFORE LOADING MODEL EXCEPT----------------------------\")\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "                                                          quantization_config = self.bnb_config, low_cpu_mem_usage=True)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "                                                  \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "                                                  padding_side=PADDING_SIDE,\n",
        "                                                  add_eos_token=ADD_EOS_TOKEN,\n",
        "                                                  add_bos_token=ADD_BOS_TOKEN,\n",
        "                                                  )\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        self.tokenizer = tokenizer\n",
        "        print(\"-------------------------AFTER LOADING MODEL EXCEPT----------------------------\")\n",
        "\n",
        "    elif self.model_name_choice == \"meta-llama/Llama-2-7b-chat\":\n",
        "      try:\n",
        "        print(\"-------------------------BEFORE LOADING MODEL LOCAL TRY----------------------------\")\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\"model/llama/\", quantization_config = self.bnb_config, low_cpu_mem_usage=True)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "                                                  \"model/llama/\",\n",
        "                                                  padding_side=PADDING_SIDE,\n",
        "                                                  add_eos_token=ADD_EOS_TOKEN,\n",
        "                                                  add_bos_token=ADD_BOS_TOKEN,\n",
        "                                                  )\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        self.tokenizer = tokenizer\n",
        "        print(\"-------------------------AFTER LOADING MODEL LOCAL TRY----------------------------\")\n",
        "      except:\n",
        "        print(\"-------------------------BEFORE LOADING MODEL EXCEPT----------------------------\")\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat\", quantization_config = self.bnb_config, low_cpu_mem_usage=True)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "                                                  \"meta-llama/Llama-2-7b-chat\",\n",
        "                                                  padding_side=PADDING_SIDE,\n",
        "                                                  add_eos_token=ADD_EOS_TOKEN,\n",
        "                                                  add_bos_token=ADD_BOS_TOKEN,\n",
        "                                                  )\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        self.tokenizer = tokenizer\n",
        "        print(\"-------------------------AFTER LOADING MODEL EXCEPT----------------------------\")\n",
        "\n",
        "  def success_msg_load(self):\n",
        "    return \"Successfully load Model and Tokenizer\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGetLThQ59hs"
      },
      "source": [
        "# Pipeline and LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8cZH1g7whVi"
      },
      "outputs": [],
      "source": [
        "class Pipeline_and_llm(Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.qa = None\n",
        "\n",
        "  def create_pipeline(self):\n",
        "    pipe = pipeline(\n",
        "        model=self.model,\n",
        "        task='text-generation',\n",
        "        tokenizer=self.tokenizer,\n",
        "        temperature=self.temperature,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "        max_new_tokens=self.max_new_tokens,  # mex number of tokens to generate in the output\n",
        "        repetition_penalty=self.repetition_penalty,  # without this output begins repeating\n",
        "        top_k=self.top_k,\n",
        "        top_p=self.top_p,\n",
        "    )\n",
        "    self.pipeline = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "\n",
        "  def prompt_template(self):\n",
        "    if self.prompt_choices == \"mistral_prompt\":\n",
        "      template_history = \"\"\"Context information is below.\n",
        "                            ---------------------\n",
        "                            {context}\n",
        "                            ---------------------\n",
        "                            Given the context information and not prior knowledge, answer the query.\n",
        "                            Query: {question}\n",
        "                            Answer:\"\"\"\n",
        "\n",
        "      template_diagnose = \"\"\"[INST] You are a medical assistant chatbot having a conversation with human. Answer the question based on the context below, and if the question can't be answered based on the context, say \"I don't know\".\n",
        "                    Context: {context}\n",
        "                    Human: {question}\n",
        "                    Chatbot:[/INST]\"\"\"\n",
        "      self.history_prompt = PromptTemplate.from_template(template_history)\n",
        "      self.diagnose_prompt = PromptTemplate.from_template(template_diagnose)\n",
        "\n",
        "    elif self.prompt_choices == \"llama_prompt\":\n",
        "      template_history = \"\"\"[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.<</SYS>>\n",
        "                    Question: {question}\n",
        "                    Context: {context}\n",
        "                    Answer: [/INST]\"\"\"\n",
        "\n",
        "      template_diagnose = \"\"\"[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.<</SYS>>\n",
        "                    Question: {question}\n",
        "                    Context: {context}\n",
        "                    Answer: [/INST]\"\"\"\n",
        "      self.history_prompt = PromptTemplate.from_template(template_diagnose)\n",
        "      self.diagnose_prompt = PromptTemplate.from_template(template_diagnose)\n",
        "\n",
        "    elif self.prompt_choices == \"restrict\":\n",
        "      template_history = \"\"\"[INST]You are a very helpful Medical health assistant. Your goal is to answer each question, using the following documents as context, as truthfully as you can. If you cannot answer the question or find relevant meaning in the presented texts, tell the user to try re-phrasing the question.\n",
        "                    Context: {context}\n",
        "                    History: {chat_history}\n",
        "                    Question: {question}\n",
        "                    Answer: [/INST]\"\"\"\n",
        "\n",
        "      template_diagnose = \"\"\"[INST]You are a very helpful Medical health assistant. Your goal is to answer each question, using the following documents as context, as truthfully as you can. If you cannot answer the question or find relevant meaning in the presented texts, tell the user to try re-phrasing the question.\n",
        "                    Context: {context}\n",
        "                    Question: {question}\n",
        "                    Answer: [/INST]\"\"\"\n",
        "      self.history_prompt = PromptTemplate.from_template(template_history)\n",
        "      self.diagnose_prompt = PromptTemplate.from_template(template_diagnose)\n",
        "\n",
        "    return self.history_prompt\n",
        "\n",
        "\n",
        "  def llm_chain(self):\n",
        "    #------------------------Diagnose prompt------------------------#\n",
        "    self.diagnose_chain = LLMChain(llm=self.pipeline, prompt=self.diagnose_prompt)\n",
        "\n",
        "    self.memory = ConversationSummaryBufferMemory(\n",
        "    llm=self.pipeline, memory_key=\"chat_history\", return_messages=True, max_token_limit=1000\n",
        "    )\n",
        "\n",
        "\n",
        "  def retriever_chain(self):\n",
        "    self.retrieval_chain = (\n",
        "      {\"context\": self.local_retriever, \"question\": RunnablePassthrough()}\n",
        "      | self.diagnose_chain\n",
        "    )\n",
        "\n",
        "  def create_guardrails(self):\n",
        "    print(\"---------------------------CREATE CONFIG-------------------------------------\")\n",
        "    self.config = RailsConfig.from_content(RAG_COLANG_CONTENT)\n",
        "    self.guardrails = RunnableRails(self.config)\n",
        "    print(\"---------------------------CREATE CONFIG DONE-------------------------------------\")\n",
        "\n",
        "  def history_retriever_chain(self, query):\n",
        "    condense_prompt = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. Preserve the original question in the answer sentiment during rephrasing. If the question is not related to medical health. Just say \"Sorry! I don't know.\n",
        "    Chat History:\n",
        "    {chat_history}\n",
        "    Follow Up Input: {question}\n",
        "    Standalone question:\"\"\"\n",
        "    question_generator_prompt = PromptTemplate.from_template(condense_prompt)\n",
        "    self.question_generator_chain = LLMChain(llm=self.pipeline, prompt=question_generator_prompt)\n",
        "\n",
        "\n",
        "    self.qa = ConversationalRetrievalChain.from_llm(\n",
        "        self.pipeline,\n",
        "        retriever=self.local_retriever,\n",
        "        memory=self.memory,\n",
        "        verbose=True,\n",
        "        combine_docs_chain_kwargs={\"prompt\": self.history_prompt}\n",
        "    )\n",
        "    # self.chain_with_guardrails = RunnableRails(self.config, runnable=self.qa)\n",
        "\n",
        "\n",
        "\n",
        "  def success_msg_pipeline(self):\n",
        "    return \"Successfully load model, tokenizer. Created Pipeline and chain\"\n",
        "\n",
        "\n",
        "  def diagnose_inference(self, query):\n",
        "    self.query = query\n",
        "\n",
        "    if self.query is None:\n",
        "        raise ValueError(\"Query is not set. Please provide a query.\")\n",
        "    answer = self.retrieval_chain.invoke(self.query)\n",
        "    # print(answer)\n",
        "    context = answer.get(\"context\")\n",
        "    context_processed = [context[i].page_content.replace(\"\\n\", \" \") for i in range(len(context))]\n",
        "    meta_data = [context[i].metadata for i in range(len(context))]\n",
        "    # post_processed_answer = answer[\"text\"].split(\"Helpful Answer:\")[-1].strip()\n",
        "    return context_processed, meta_data, answer.get(\"text\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdijcG1t6NqJ"
      },
      "source": [
        "# Instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxG8QnpUx6kK"
      },
      "outputs": [],
      "source": [
        "Chatbot = Pipeline_and_llm()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hr4GfF1ZvpqV"
      },
      "outputs": [],
      "source": [
        "def predict(message, history):\n",
        "  global Chatbot\n",
        "  gpt_response = Chatbot.qa(message)\n",
        "  retrieve_content = Chatbot.local_retriever.invoke(message)\n",
        "\n",
        "  if len(retrieve_content) == 0:\n",
        "    preprocess_hist = gpt_response[\"chat_history\"]\n",
        "    if type(preprocess_hist[-1]) == langchain_core.messages.ai.AIMessage:\n",
        "      gpt_response[\"chat_history\"][-1].content = FIX_RESPONSE\n",
        "      gpt_response[\"answer\"] = FIX_RESPONSE\n",
        "    return FIX_RESPONSE\n",
        "\n",
        "  else:\n",
        "    return gpt_response[\"answer\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKJcATIDPU2j"
      },
      "source": [
        "# Login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d35qmWHAQzuE"
      },
      "outputs": [],
      "source": [
        "class login_setup:\n",
        "  def HF_login(self, hf_token):\n",
        "    self.hf_token = hf_token\n",
        "    login(token = self.hf_token, add_to_git_credential=True)\n",
        "    return \"Successfully Login into HuggingFace\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wQ3Th9UU-2U"
      },
      "outputs": [],
      "source": [
        "LOGIN = login_setup()\n",
        "LOGIN.HF_login(HUGGINGFACE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# manually create vectordb\n",
        "\n",
        "Learning Disability.pdf has error"
      ],
      "metadata": {
        "id": "AApoVFfyjbqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chatbot.loading_dataset()\n",
        "# file_extension, directory, _ = Chatbot.get_file_types()\n",
        "# print(f\"file Extention : {file_extension}\")\n",
        "# print(\"-------------------------------------\")\n",
        "# print(f\"number of files : {directory}\")\n",
        "# print(\"-------------------------------------\")\n",
        "# print(f\"Size : {_}\")\n",
        "# print(\"-------------------------------------\")\n"
      ],
      "metadata": {
        "id": "QyyiRRrPehR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chatbot.create_vector_store_index(file_extension)\n",
        "# Chatbot.load_embedding_model()\n",
        "# Chatbot.create_db()"
      ],
      "metadata": {
        "id": "QikELCI8f-Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JexreQeLRfi6"
      },
      "source": [
        "# UI\n",
        "\n",
        "Generate Long answer due to conversationbuffermemory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "css = \"\"\"\n",
        ".feedback {margin-top: 140px !important}\n",
        ".file_st {height: 50px; width: 10px;}\n",
        ".feed {height: 25px; width: 70px; margin-top: 10px}\n",
        ".fortextbox {width: 70px;}\n",
        ".forprompt {height: 25px; width: 70px; margin-top: 200px}\n",
        ".forparameter {margin-top: 50px}\n",
        ".forqabutton {margin-top: 65px}\n",
        "\"\"\"\n",
        "#  margin-left: 80px; margin-right: 150px;"
      ],
      "metadata": {
        "id": "2-OW_FMZiUKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Erp1SKcALooh"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks(theme = \"finlaymacklon/smooth_slate\", css=css) as demo:\n",
        "  gr.Markdown(\"\"\"\n",
        "                # Mental health assistant\n",
        "              \"\"\")\n",
        "\n",
        "  with gr.Row(equal_height=False):\n",
        "    # UI for Model Parameters\n",
        "    with gr.Column(scale = 16):\n",
        "      model_name = gr.Dropdown(choices = Chatbot.model_name_choice, label = \"Select a model\", info = \"Please select model. i.e mistralai/Mistral-7B-v0.1\", value = Chatbot.model_name_choice[0])\n",
        "      model_and_tokenizer_load = gr.components.Textbox(label = \"Message\")\n",
        "    with gr.Column(scale = 4, elem_classes=\"feedback\"):\n",
        "      model_and_tokenizer_load_btn = gr.Button(\"Reload\", size='lg', min_width=10)\n",
        "      model_and_tokenizer_load_btn.click(Chatbot.set_model_name_choice, inputs=[model_name]).then(Chatbot.set_config).then(Chatbot.load_model\n",
        "                                    ).then(Chatbot.create_pipeline).then(Chatbot.success_msg_load, outputs=model_and_tokenizer_load, show_progress=True)\n",
        "\n",
        "  with gr.Tab(\"Inference\"):\n",
        "\n",
        "\n",
        "    with gr.Tab(\"QA with history\"):\n",
        "      with gr.Row():\n",
        "        with gr.Column():\n",
        "          inference_checkbox = gr.Checkbox(label=\"Only look for answer in provided private data\")\n",
        "          chat = gr.ChatInterface(predict,\n",
        "                                  chatbot=gr.Chatbot(render=False, height=1000))\n",
        "\n",
        "    with gr.Tab(\"Diagnostic QA\"):\n",
        "      with gr.Row(equal_height=False):\n",
        "        with gr.Column(scale=16):\n",
        "          inference_input = gr.components.Textbox(label = \"Query\", info = \"Write query related to your docs.\")\n",
        "        with gr.Column(scale=4, elem_classes=\"forqabutton\"):\n",
        "          btn = gr.Button(\"Generate\")\n",
        "      with gr.Row():\n",
        "        with gr.Column():\n",
        "          inference_output_question = gr.components.Textbox(label = \"Relevant Text\", lines=5)\n",
        "          inference_output_context = gr.components.Textbox(label = \"Metadata\", lines=3)\n",
        "          inference_output_text = gr.components.Textbox(label = \"Answer\")\n",
        "\n",
        "      btn.click(Chatbot.diagnose_inference, inputs=[inference_input], outputs=[inference_output_question, inference_output_context, inference_output_text], show_progress=True)\n",
        "\n",
        "\n",
        "\n",
        "  with gr.Tab(\"Prompts and Data Store\"):\n",
        "\n",
        "    with gr.Row(equal_height=False):\n",
        "      with gr.Column(scale=16):\n",
        "        prompt = gr.Dropdown(choices = Chatbot.prompt_choices, label = \"Select Prompt\", info = \"Please provide your own prompt template if you want.\", value = Chatbot.prompt_choices[0])\n",
        "        prompt_text = gr.components.Textbox(label = \"Prompt Text\", lines = 4)\n",
        "      with gr.Column(scale=2, elem_classes=\"forprompt\"):\n",
        "        prompts_btn = gr.Button(\"Set Prompt\")\n",
        "        prompts_btn.click(Chatbot.set_prompt_choices, inputs=[prompt]).then(Chatbot.llm_chain).then(Chatbot.retriever_chain\n",
        "                        ).then(Chatbot.history_retriever_chain).then(Chatbot.prompt_template, outputs=[prompt_text])\n",
        "\n",
        "\n",
        "    gr.Markdown(\"File must be PDF\")\n",
        "    gr.Markdown(\"Please update chunk size and chunk overlap if you want to change it according to your use case.\")\n",
        "    with gr.Row(equal_height=False):\n",
        "\n",
        "      with gr.Column(scale = 16, elem_classes=\"file_st\"):\n",
        "        files_upload = gr.File(type='filepath', file_count=\"multiple\")\n",
        "\n",
        "      with gr.Column(scale = 2, elem_classes=\"feed\"):\n",
        "        data_btn = gr.Button(\"Reindex\")\n",
        "\n",
        "\n",
        "    with gr.Column():\n",
        "      file_types_string = gr.components.Textbox(label = \"File type\")\n",
        "      number_of_files = gr.components.Textbox(label = \"Number of files\")\n",
        "      size_of_files = gr.components.Textbox(label = \"Size of files\")\n",
        "      data_success_msg_db = gr.components.Textbox(label = \"Message\")\n",
        "\n",
        "      data_btn.click(Chatbot.set_files, inputs=[files_upload]).then(Chatbot.loading_dataset).then(Chatbot.get_file_types, outputs=[file_types_string, number_of_files, size_of_files]\n",
        "                    ).then(Chatbot.create_vector_store_index).then(Chatbot.load_embedding_model).then(Chatbot.load_db).then(Chatbot.reindex_db).then(Chatbot.llm_chain).then(Chatbot.retriever_chain\n",
        "                    ).then(Chatbot.history_retriever_chain\n",
        "                    ).then(Chatbot.success_msg_data_db, outputs=data_success_msg_db, show_progress=True)\n",
        "\n",
        "\n",
        "  with gr.Tab(\"Parameters\"):\n",
        "\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        embedding_model = gr.components.Textbox(label = \"Embedding Model\", info = \"Please provide a embedding model from Huggingface.\", value = \"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "    gr.Markdown(\"## Model Parameters\")\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        temperature = gr.Slider(minimum = 0, maximum = 1, label = \"Temperature\", info = \" Close to 1 general, close to zero specific to data.\", value = 0.2, step = 0.1)\n",
        "        max_new_tokens = gr.Slider(minimum = 100, maximum = 1024, label = \"Max New Tokens\", info = \"Number of max new token you wan.\", value = 100, step = 1)\n",
        "        repetition_penalty = gr.Slider(minimum = 0.1, maximum = 100, label = \"Repetition Penalty\", info = \"\", value = 1.15, step = 0.01)\n",
        "\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        top_k = gr.Slider(minimum = 1, maximum = 1000, label = \"Top k\", info = \"\", value = 100, step = 1)\n",
        "        top_p = gr.Slider(minimum = 0.1, maximum = 1.0, label = \"Top p\", info = \"\", value = 0.95, step = 0.01)\n",
        "\n",
        "    gr.Markdown(\"## Data Parameters\")\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        k_context = gr.Slider(minimum = 1, maximum = 10, label = \"K context\", info = \"\", value = 2, step = 1)\n",
        "        chunk_size = gr.Slider(minimum = 100, maximum = 2048, label = \"Chunk Size\", info = \"\", value = 1024, step = 1)\n",
        "        chunk_overlap = gr.Slider(minimum = 100, maximum = 2048, label = \"Chunk Overlap\", info = \"\", value = 100, step = 1)\n",
        "\n",
        "    with gr.Row(equal_height=False):\n",
        "      with gr.Column(scale=16):\n",
        "        success_message = gr.components.Textbox(label = \"Message\")\n",
        "      with gr.Column(scale=4, elem_classes=\"forparameter\"):\n",
        "        parameters_btn = gr.Button(\"Set Parameter\")\n",
        "    parameters_btn.click(Chatbot.set_embedding_model, inputs = [embedding_model]).then(Chatbot.set_temperature, inputs = [temperature]\n",
        "                        ).then(Chatbot.set_max_new_tokens, inputs = [max_new_tokens]).then(Chatbot.set_repetition_penalty, inputs = [repetition_penalty]\n",
        "                        ).then(Chatbot.set_top_k, inputs = [top_k]).then(Chatbot.set_k_context, inputs=[k_context]).then(Chatbot.set_chunk_size, inputs = [chunk_size]).then(Chatbot.set_chunk_overlap, inputs = [chunk_overlap]\n",
        "                        ).then(Chatbot.create_pipeline).then(Chatbot.llm_chain).then(Chatbot.retriever_chain\n",
        "                        ).then(Chatbot.history_retriever_chain).then(Chatbot.success_msg_params, outputs=[success_message])\n",
        "\n",
        "\n",
        "        #----------------------------------------------------------------------------------------------------------------------------------------------------#\n",
        "  demo.load(Chatbot.set_model_name_choice, inputs=[model_name]).success(Chatbot.set_config).success(Chatbot.load_model\n",
        "            ).success(Chatbot.create_pipeline).success(Chatbot.success_msg_load, outputs=model_and_tokenizer_load, show_progress=True\n",
        "            ).success(Chatbot.set_prompt_choices, inputs=[prompt]).success(Chatbot.prompt_template, outputs=[prompt_text]\n",
        "            ).success(Chatbot.load_embedding_model).success(Chatbot.load_db).success(Chatbot.llm_chain).success(Chatbot.retriever_chain\n",
        "            ).success(Chatbot.history_retriever_chain).success(Chatbot.success_msg_data_db, outputs=data_success_msg_db, show_progress=True)\n",
        "  #----------------------------------------------------------------------------------------------------------------------------------------------------#\n",
        "if __name__ == \"__main__\":\n",
        "  demo.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-mAB8U8_WGS"
      },
      "source": [
        "# Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrLfw6PN_Wy3"
      },
      "outputs": [],
      "source": [
        "# def get_score(self, query):\n",
        "#   docs = self.vectordb.similarity_search_with_relevance_scores(query)\n",
        "#   print(docs)\n",
        "#   # similarity_scores = docs.similarity_scores\n",
        "#   # for i, doc in enumerate(docs):\n",
        "#   #   print(f\"Document {i+1}:\")\n",
        "#   #   print(f\"Content: {doc.page_content}\\n\")\n",
        "#   #   print(f\"Similarity Score: {doc.similarity_scores[i]}\\n\")\n",
        "#   return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmaU-jZv_XTy"
      },
      "outputs": [],
      "source": [
        "# docs = Chatbot.vectordb.similarity_search_with_relevance_scores(\"What is mental health?\")\n",
        "# [item for item in docs if item[1] > 0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DirectoryLoader??"
      ],
      "metadata": {
        "id": "EsOroXXOhE3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# s = \"\"\"[INST] You are an Computer Science Conversational assistant. You strickly answers from provided context only. if no context is found. you politely refuse.\n",
        "# Context: 029 2068 8401 @ncmh_wales (\n",
        "\n",
        "# Being a teenager can be tough, but it shouldn’t feel hopeless. If you\n",
        "# have been feeling sad most of the time for a few weeks or longer and you’re not able to concentrate or do the things you used to enjoy, talk to a trusted adult about depression.\n",
        "# You’re not alone, and help is avail able. You can feel better.\n",
        "# To get help, call or text the 988 Suicide & Crisis Lifeline at 988 or chat a t 988lifeline.org .\n",
        "# nimh.nih.gov/depression\n",
        "# NIMH Identifier No. OM 22-4321\n",
        "\n",
        "# What is depression?\n",
        "# Everyone feels sad or low sometimes, but these feelings usually pass with a little time.\n",
        "\n",
        "# 10 GET EXCITED ABOUT MENTAL HEALTH RESEARCH! 18 12 8 424 80 33 12 24 80 433 56 12 212 24 56 16 33 1256A Cryptic Message About\n",
        "# Your Future\n",
        "# Solve the math problems, then use the letters below your answers to decode\n",
        "# the hidden message in the blanks below.\n",
        "# 12 + 12 5 x 0 8 + 8 16 + 16 6 + 6 1 x 9 30 - 2 25 + 8 5 + 1 90 - 9 55 + 8 60 + 20 10 + 8\n",
        "# A B C D E F G H I J K L M\n",
        "# 4 + 4 15 + 15 30 - 5 9 + 6 50 + 6 2 x 1 2 + 2 30 - 3 10 + 10 3 x 1 3 + 4 7 + 7 24 + 2\n",
        "# N O P Q R S T U V W X Y Z\n",
        "# History:\n",
        "# Human: I am Huzaifa. I am feeling lost for few days.\n",
        "# Chatbot:[/INST] Hello Huzaifa, I'm sorry to hear that you've been feeling lost for a few days. Is there anything specific that's troubling you that you would like to talk about?\"\"\""
      ],
      "metadata": {
        "id": "F4bDPb17hFRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# s.split(\"Chatbot:[/INST]\")[-1]"
      ],
      "metadata": {
        "id": "glVtsjxNDySP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ConversationBufferMemory??"
      ],
      "metadata": {
        "id": "hdN-e4oeD3Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip freeze | grep gradio"
      ],
      "metadata": {
        "id": "z-hWjv2X6aF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Az-vAPzw0846"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loadDB.similarity_search(\"What is depression?\")"
      ],
      "metadata": {
        "id": "POCw0yvi6lrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # def history_retriever_chain(self):\n",
        "  #   condense_prompt = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. Preserve the original question in the answer sentiment during rephrasing. If the question is not related to medical health. Just say \"Sorry! I don't know.\n",
        "  #   Chat History:\n",
        "  #   {chat_history}\n",
        "  #   Follow Up Input: {question}\n",
        "  #   Standalone question:\"\"\"\n",
        "  #   CONDENSE_QUESTION_PROMPT_CUSTOM = PromptTemplate.from_template(condense_prompt)\n",
        "  #   self.qa = RetrievalQA.from_chain_type(\n",
        "  #       self.application.llm,\n",
        "  #       retriever=self.local_retriever_qa,\n",
        "  #       memory=self.memory,\n",
        "  #       verbose=True,\n",
        "  #       combine_docs_chain_kwargs={\"prompt\": self.history_prompt},\n",
        "  #       # response_if_no_docs_found = \"Sorry! I don't know.\",\n",
        "  #       # rephrase_question = False,\n",
        "  #       # condense_question_prompt=CONDENSE_QUESTION_PROMPT_CUSTOM\n",
        "  #   )\n",
        "  #   self.application.register_action(self.qa, name=\"qa_chain\")"
      ],
      "metadata": {
        "id": "4X8N-g4_6p3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PeftModel.from_pretrained??"
      ],
      "metadata": {
        "id": "xhumRCGCrUc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NTAM-fW_rU80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # if self.prompt_choices == \"mistral_prompt\":\n",
        "    #   template_history = \"\"\"[INST] You are a medical assistant chatbot having a conversation with human. Answer the question based on the context below, and if the question can't be answered based on the context, say \"I don't know\".\n",
        "    #                 Context: {context}\n",
        "    #                 History: {chat_history}\n",
        "    #                 Human: {question}\n",
        "    #                 Chatbot:[/INST]\"\"\"\n",
        "\n",
        "    #   template_diagnose = \"\"\"[INST] You are a medical assistant chatbot having a conversation with human. Answer the question based on the context below, and if the question can't be answered based on the context, say \"I don't know\".\n",
        "    #                 Context: {context}\n",
        "    #                 Human: {question}\n",
        "    #                 Chatbot:[/INST]\"\"\"\n",
        "    #   self.history_prompt = PromptTemplate.from_template(template_history)\n",
        "    #   self.diagnose_prompt = PromptTemplate.from_template(template_diagnose)"
      ],
      "metadata": {
        "id": "2woPW1oHEMUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1"
      ],
      "metadata": {
        "id": "Dzl2hyxU2Cvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chatbot.set_model_name_choice(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
        "# Chatbot.set_prompt_choices(\"mistral_prompt\")\n",
        "# Chatbot.set_config()\n",
        "# Chatbot.load_model()\n",
        "# Chatbot.create_pipeline()\n",
        "# Chatbot.prompt_template()\n",
        "# Chatbot.load_embedding_model()\n",
        "# Chatbot.load_db()\n",
        "# Chatbot.llm_chain()\n",
        "# Chatbot.retriever_chain()\n",
        "# Chatbot.create_guardrails()"
      ],
      "metadata": {
        "id": "Fwtf0KRB9kMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chatbot.diagnose_inference(\"What does Glycogen Phosphorylase do?\")\n",
        "# query = \"What is SAD?\""
      ],
      "metadata": {
        "id": "yIAY0-wz7FhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chatbot.chain_with_guardrails.get_prompts()\n",
        "# import nest_asyncio\n",
        "# nest_asyncio.apply()\n",
        "# Chatbot.qa({\"question\" : query})"
      ],
      "metadata": {
        "id": "NenvQ3-U7hro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[HumanMessage(content='What would I experience during a clinical trial?'),\n",
        "\n",
        "AIMessage(content=' During a clinical trial, the study team will track your health. Participating in a clinical trial may take more time than standard treatment, and you may have more tests and treatments than you would if you weren’t in a clinical trial. The study team also may ask you to keep a log of symptoms or other health measures, fill out forms about how you feel, or complete other tasks. You may need to travel or reside away from home to take part in a study. Clinical trials are research studies that look at new ways to prevent, detect, or treat diseases and conditions. Although individuals may benefit from being part of a')]"
      ],
      "metadata": {
        "id": "CX54Q9lEbw3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# La = [HumanMessage(content='What would I experience during a clinical trial?'),\n",
        "#       AIMessage(content=' During a clinical trial, the study team will track your health. Participating in a clinical trial may take more time than standard treatment, and you may have more tests and treatments than you would if you weren’t in a clinical trial. The study team also may ask you to keep a log of symptoms or other health measures, fill out forms about how you feel, or complete other tasks. You may need to travel or reside away from home to take part in a study. Clinical trials are research studies that look at new ways to prevent, detect, or treat diseases and conditions. Although individuals may benefit from being part of a')\n",
        "#       ]"
      ],
      "metadata": {
        "id": "2CUJnYG8bzSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [i.content for i in La if type(i) == langchain_core.messages.ai.AIMessage]"
      ],
      "metadata": {
        "id": "ZPaoi_nCqgcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in La:\n",
        "#   if type(i) == langchain_core.messages.ai.AIMessage:\n",
        "#     i.content = \"I don't know\""
      ],
      "metadata": {
        "id": "n3O6VFz7ezUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AI = La[1]"
      ],
      "metadata": {
        "id": "QRJEafxcb_RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def predict(message, history):\n",
        "#   global Chatbot\n",
        "#   history_langchain_format = []\n",
        "#   for human, ai in history:\n",
        "#     history_langchain_format.append(HumanMessage(content=human))\n",
        "#     history_langchain_format.append(AIMessage(content=ai))\n",
        "#   history_langchain_format.append(HumanMessage(content=message))\n",
        "#   gpt_response = Chatbot.qa(message)         # , \"chat_history\": history_langchain_format\n",
        "#   retrieve_content = Chatbot.local_retriever.invoke(message)\n",
        "#   print(\"------------------------RETRIEVE CONTENT-----------------------------------------------------\")\n",
        "#   print(retrieve_content)\n",
        "#   # print(\"---------------------------------------HISTORY----------------------------------------------\")\n",
        "#   print(type(history_langchain_format))\n",
        "#   print(\"------------------------------------------------------------------------------------------\")\n",
        "#   print(gpt_response)\n",
        "#   print(\"------------------------------------------------------------------------------------------\")\n",
        "#   # print(\"------------------------------------------------------------------------------------------\")\n",
        "#   return gpt_response[\"answer\"]  # .split(\"Chatbot\")[-1]"
      ],
      "metadata": {
        "id": "pyfqmEpnrS8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# alp = {'question': 'What is SAD?', 'chat_history': [HumanMessage(content='What is sad'), AIMessage(content=\" Sadness is a normal human emotion that is experienced when something disappointing, upsetting, or threatening happens. It's also known as depression. Everyone feels sad or low sometimes, but these feelings usually pass with a little time. However, if someone experiences persistent sadness or loss of interest or pleasure in activities for several weeks or longer, it may be a sign of depression. Depression is a serious illness that affects millions of people worldwide. There are different types of depression, including major depressive disorder, persistent depressive disorder, postpartum depression, seasonal affective disorder, and bipolar disorder. Symptoms\"), HumanMessage(content='What is SAD?'), AIMessage(content='  Seasonal Affective Disorder (SAD) is a type of depression characterized by a recurrent seasonal pattern, with symptoms lasting about 4−5 months out of the year. It is more common in people with depression or bipolar disorder, especially bipolar II disorder. Because SAD, like other types of depression, is associated with disturbances in serotonin activity, antidepressant medications called selective serotonin reuptake inhibitors are sometimes used to treat symptoms.')], 'answer': '  Seasonal Affective Disorder (SAD) is a type of depression characterized by a recurrent seasonal pattern, with symptoms lasting about 4−5 months out of the year. It is more common in people with depression or bipolar disorder, especially bipolar II disorder. Because SAD, like other types of depression, is associated with disturbances in serotonin activity, antidepressant medications called selective serotonin reuptake inhibitors are sometimes used to treat symptoms.'}\n"
      ],
      "metadata": {
        "id": "SNIRPlratmRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# alp[\"chat_history\"]"
      ],
      "metadata": {
        "id": "l3uRYixHuCgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ConversationalRetrievalChain??"
      ],
      "metadata": {
        "id": "FlFTo0gUuDlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D_ymp5HdyTSH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "voZv9isaAExl",
        "km2ornvJ9ft5",
        "4XtOACw0PSD2",
        "GlyEzEOWPQJs",
        "hSQ4YYeXLAzP",
        "XCSzcOaMK98c",
        "i_JMi8tILECh",
        "qGetLThQ59hs",
        "tdijcG1t6NqJ",
        "RKJcATIDPU2j",
        "AApoVFfyjbqd",
        "JexreQeLRfi6",
        "I-mAB8U8_WGS",
        "Dzl2hyxU2Cvt"
      ],
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}