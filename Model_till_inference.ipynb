{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "qkRws5Qj9X67"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0_gqU219T4qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91de8043-8db0-47bd-b6d6-7fc99409058b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.9/307.9 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.5/258.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q gradio wandb\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U datasets scipy\n",
        "!pip install -q evaluate rouge_score\n",
        "!pip install -q ipywidgets==7.7.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dependencies"
      ],
      "metadata": {
        "id": "SYamZRYg9bIg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aMd0k5w8cIe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e20a3e50-936d-4d30-87a8-38f00b63617e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, TrainingArguments, DataCollatorForLanguageModeling, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
        "from peft import PeftModel\n",
        "from threading import Thread\n",
        "import wandb\n",
        "import os\n",
        "import gradio as gr\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "import numpy as np\n",
        "# for config\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
        "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
        "from huggingface_hub import login\n",
        "from datasets import load_dataset, Dataset\n",
        "# Metrics library\n",
        "import evaluate\n",
        "# Warning libraries\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global/Config variables"
      ],
      "metadata": {
        "id": "IqUJE8Qc3mkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS = \"rouge\"\n",
        "LOAD_IN_4BIT = True\n",
        "BNB_4BIT_USE_DOUBLE_QUANT = True\n",
        "BNB_4BIT_QUANT_TYPE = \"nf4\"\n",
        "ADD_EOS_TOKEN = \"[PAD]\"\n",
        "PADDING_SIDE = \"left\"\n",
        "ADD_BOS_TOKEN = True\n",
        "HUGGINGFACE_API_KEY = \"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\"\n",
        "WANDB_API_KEY = \"8fed25a36c785fb869426ffff16b98259a4ecec3\""
      ],
      "metadata": {
        "id": "J_SbsJsIPWtB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metric = evaluate.load(METRICS)"
      ],
      "metadata": {
        "id": "p6IAoqCPiysS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP7GJQ-fJSwB"
      },
      "source": [
        "# parameter class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "h0ZEd1zIU8Ff"
      },
      "outputs": [],
      "source": [
        "class Parameters:\n",
        "  def __init__(self, dataset_name = \"Amod/mental_health_counseling_conversations\", data_split = \"train\", question_col_name = \"Context\", answer_col_name = \"Response\", train_size = 0.2,\n",
        "               r = 32, lora_alpha = 64, lora_dropout = 0.05, model_name = \"mistralai/Mistral-7B-v0.1\", warmup_steps = 10, per_device_train_batch_size = 4, per_device_eval_batch_size = 4,\n",
        "               gradient_accumulation_steps = 1, max_steps = 100, learning_rate = 2.5e-5, fp16 = True, optimizer = \"paged_adamw_8bit\", logging_steps = 50, save_steps = 50, eval_steps = 50,\n",
        "               do_eval = True, report_to = \"wandb\", checkpoint_to_be_saved = 100, wandb_project_name = \"huggingface\"):\n",
        "    # Dataset Parameters\n",
        "    self.dataset_name = dataset_name\n",
        "    self.data_split = data_split\n",
        "    self.question_col_name = question_col_name\n",
        "    self.answer_col_name = answer_col_name\n",
        "    self.train_size = train_size\n",
        "\n",
        "    # Config Parameters\n",
        "    self.r = r                     # LoRA config\n",
        "    self.lora_alpha = lora_alpha             # LoRA config\n",
        "    self.lora_dropout = lora_dropout\n",
        "\n",
        "\n",
        "    # Model Parameters\n",
        "    self.model_name = model_name\n",
        "\n",
        "    # Training Parameters\n",
        "    self.warmup_steps = warmup_steps\n",
        "    self.per_device_train_batch_size = per_device_train_batch_size\n",
        "    self.per_device_eval_batch_size = per_device_eval_batch_size\n",
        "    self.gradient_accumulation_steps = gradient_accumulation_steps\n",
        "    self.max_steps = max_steps\n",
        "    self.learning_rate = learning_rate\n",
        "    self.fp16 = fp16\n",
        "    self.optimizer = optimizer\n",
        "    self.logging_steps = logging_steps\n",
        "    self.save_steps = save_steps\n",
        "    self.eval_steps = eval_steps\n",
        "    self.do_eval = do_eval\n",
        "    self.report_to = report_to\n",
        "    self.checkpoint_to_be_saved = checkpoint_to_be_saved\n",
        "\n",
        "    # Wandb Parameter\n",
        "    self.wandb_project_name = wandb_project_name\n",
        "\n",
        "  # Functions to modify variables\n",
        "  def set_dataset_name(self, dataset_name):\n",
        "      self.dataset_name = dataset_name\n",
        "\n",
        "  def set_data_split(self, data_split):\n",
        "      self.data_split = data_split\n",
        "\n",
        "  def set_question_col_name(self, question_col_name):\n",
        "      self.question_col_name = question_col_name\n",
        "\n",
        "  def set_answer_col_name(self, answer_col_name):\n",
        "      self.answer_col_name = answer_col_name\n",
        "\n",
        "  def set_train_size(self, train_size):\n",
        "      self.train_size = train_size\n",
        "\n",
        "  def set_r(self, r):\n",
        "      self.r = r\n",
        "\n",
        "  def set_lora_alpha(self, lora_alpha):\n",
        "      self.lora_alpha = lora_alpha\n",
        "\n",
        "  def set_lora_dropout(self, lora_dropout):\n",
        "      self.lora_dropout = lora_dropout\n",
        "\n",
        "  def set_model_name(self, model_name):\n",
        "      self.model_name = model_name\n",
        "\n",
        "  def set_warmup_steps(self, warmup_steps):\n",
        "      self.warmup_steps = warmup_steps\n",
        "\n",
        "  def set_per_device_train_batch_size(self, per_device_train_batch_size):\n",
        "      self.per_device_train_batch_size = per_device_train_batch_size\n",
        "\n",
        "  def set_per_device_eval_batch_size(self, per_device_eval_batch_size):\n",
        "      self.per_device_eval_batch_size = per_device_eval_batch_size\n",
        "\n",
        "  def set_gradient_accumulation_steps(self, gradient_accumulation_steps):\n",
        "      self.gradient_accumulation_steps = gradient_accumulation_steps\n",
        "\n",
        "  def set_max_steps(self, max_steps):\n",
        "      self.max_steps = max_steps\n",
        "\n",
        "  def set_learning_rate(self, learning_rate):\n",
        "      self.learning_rate = learning_rate\n",
        "\n",
        "  def set_fp16(self, fp16):\n",
        "      self.fp16 = fp16\n",
        "\n",
        "  def set_optimizer(self, optimizer):\n",
        "      self.optimizer = optimizer\n",
        "\n",
        "  def set_logging_steps(self, logging_steps):\n",
        "      self.logging_steps = logging_steps\n",
        "\n",
        "  def set_save_steps(self, save_steps):\n",
        "      self.save_steps = save_steps\n",
        "\n",
        "  def set_eval_steps(self, eval_steps):\n",
        "      self.eval_steps = eval_steps\n",
        "\n",
        "  def set_do_eval(self, do_eval):\n",
        "      self.do_eval = do_eval\n",
        "\n",
        "  def set_report_to(self, report_to):\n",
        "      self.report_to = report_to\n",
        "\n",
        "  def set_checkpoint_to_be_saved(self, checkpoint_to_be_saved):\n",
        "      self.checkpoint_to_be_saved = checkpoint_to_be_saved\n",
        "\n",
        "  def set_wandb_project_name(self, wandb_project_name):\n",
        "      self.wandb_project_name = wandb_project_name\n",
        "\n",
        "  def success_msg_params(self):\n",
        "    return \"Parameters set successfully\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYg9-7fIJVqR"
      },
      "source": [
        "# Model Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JbMZobe7e_JP"
      },
      "outputs": [],
      "source": [
        "class Model(Parameters):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.config = None\n",
        "    self.project_name = \"Fine_tuned_model_dir\"\n",
        "    # self.final_model = None\n",
        "\n",
        "\n",
        "  def set_config(self):\n",
        "    self.bnb_config = BitsAndBytesConfig(\n",
        "                                        load_in_4bit = LOAD_IN_4BIT,\n",
        "                                        bnb_4bit_use_double_quant = BNB_4BIT_USE_DOUBLE_QUANT,\n",
        "                                        bnb_4bit_quant_type = BNB_4BIT_QUANT_TYPE,\n",
        "                                        bnb_4bit_compute_dtype = torch.bfloat16\n",
        ")\n",
        "  def load_model(self):\n",
        "    self.set_config()\n",
        "    self.model = AutoModelForCausalLM.from_pretrained(self.model_name, quantization_config = self.bnb_config, low_cpu_mem_usage=True)\n",
        "\n",
        "  def load_tokenizer(self):\n",
        "   self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                                              self.model_name,\n",
        "                                              padding_side=PADDING_SIDE,\n",
        "                                              add_eos_token=ADD_EOS_TOKEN,\n",
        "                                              add_bos_token=ADD_BOS_TOKEN,\n",
        "                                          )\n",
        "  #  print(self.tokenizer.add_eos_token)\n",
        "  #  print(type(self.tokenizer.add_eos_token))\n",
        "   self.tokenizer.pad_token = self.tokenizer.add_eos_token\n",
        "   return self.tokenizer\n",
        "\n",
        "  def set_peft_config(self):\n",
        "    self.model.gradient_checkpointing_enable()\n",
        "    self.model = prepare_model_for_kbit_training(self.model)\n",
        "\n",
        "    self.config = LoraConfig(\n",
        "                            r=self.r,\n",
        "                            lora_alpha=self.lora_alpha,\n",
        "                            target_modules=[\n",
        "                                \"q_proj\",\n",
        "                                \"k_proj\",\n",
        "                                \"v_proj\",\n",
        "                                \"o_proj\",\n",
        "                                \"gate_proj\",\n",
        "                                \"up_proj\",\n",
        "                                \"down_proj\",\n",
        "                                \"lm_head\",\n",
        "                            ],\n",
        "\n",
        "                            bias=\"none\",\n",
        "                            lora_dropout=self.lora_dropout,  # Conventional\n",
        "                            task_type=\"CAUSAL_LM\",\n",
        "                        )\n",
        "\n",
        "    self.model = get_peft_model(self.model, self.config)\n",
        "\n",
        "  def apply_accelerator_on_peft_model(self):\n",
        "    self.fsdp_plugin = FullyShardedDataParallelPlugin(\n",
        "                                                state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        "                                                optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        "                                            )\n",
        "    self.accelerator = Accelerator(fsdp_plugin=self.fsdp_plugin)\n",
        "    self.final_model = self.accelerator.prepare_model(self.model)\n",
        "    # return self.final_model\n",
        "\n",
        "  def success_msg_load(self):\n",
        "    return \"Successfully load Model and Tokenizer\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M844CZZchPrA"
      },
      "source": [
        "# Data Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-JAH2Qg2eFuS"
      },
      "outputs": [],
      "source": [
        "class Data(Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.padding = \"max_length\"\n",
        "\n",
        "  def test(self):\n",
        "    print(\"DataSet is \", self.dataset_name)\n",
        "\n",
        "  def loading_dataset(self):\n",
        "    try:\n",
        "        self.dataset = load_dataset(self.dataset_name, split=self.data_split)\n",
        "        print(\"------------------------------------------------load dataset fine-----------------------------------------------------------------\")\n",
        "        print(self.dataset)\n",
        "    except Exception as e:\n",
        "        print(f\"The exception from loadding_dataset : {e}\")\n",
        "        self.dataset = load_dataset(self.dataset_name)\n",
        "        print(self.dataset)\n",
        "    return self.dataset\n",
        "\n",
        "  def prepare_data(self):\n",
        "    print(\"------------------------------------------------prepare dataset fine-----------------------------------------------------------------\")\n",
        "    self.data = self.loading_dataset()\n",
        "    print(self.data)\n",
        "    self.questions = self.data[self.question_col_name]\n",
        "    self.answers = self.data[self.answer_col_name]\n",
        "\n",
        "    # Manually split the data\n",
        "    self.split_index = int(self.train_size * len(self.questions))  # Split 80% for training, 20% for evaluation\n",
        "    self.train_data = {'input_text': self.questions[:self.split_index], 'target_text': self.answers[:self.split_index]}\n",
        "    self.eval_data = {'input_text': self.questions[self.split_index:], 'target_text': self.answers[self.split_index:]}\n",
        "\n",
        "    # Create datasets without using DataFrame conversion\n",
        "    self.train_data = Dataset.from_dict(self.train_data)\n",
        "    self.eval_data = Dataset.from_dict(self.eval_data)\n",
        "    return self.train_data, self.eval_data\n",
        "\n",
        "  def preprocess_function(self, sample):\n",
        "    self.max_input_length = 512\n",
        "    self.max_target_length = 512\n",
        "    model_inputs = self.tokenizer(sample[\"input_text\"], max_length=self.max_input_length, padding=self.padding,\n",
        "                                  truncation=True)\n",
        "    labels = self.tokenizer(text_target=sample[\"target_text\"], max_length=self.max_target_length,\n",
        "                            padding=\"max_length\", truncation=True)\n",
        "    if self.padding == \"max_length\":\n",
        "        labels[\"input_ids\"] = [[(l if l != self.tokenizer.pad_token_id else -100) for l in label] for label in\n",
        "                              labels[\"input_ids\"]]\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "  def tokenized_data(self):\n",
        "    train_data, eval_data = self.prepare_data()\n",
        "    self.train_data_tokenized = train_data.map(self.preprocess_function, batched=True,\n",
        "                                                remove_columns=[\"input_text\", \"target_text\"])\n",
        "    self.eval_data_tokenized = eval_data.map(self.preprocess_function, batched=True,\n",
        "                                                remove_columns=[\"input_text\", \"target_text\"])\n",
        "    # return self.train_data, self.eval_data\n",
        "  # def send_to_train(self):\n",
        "  #   return self.train_data_tokenized, self.eval_data_tokenized\n",
        "\n",
        "  def success_msg_data(self):\n",
        "    return \"Done\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIaBG6OrhRig"
      },
      "source": [
        "# Train Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n92tiGWuWthp"
      },
      "outputs": [],
      "source": [
        "class Model_train(Data):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # self.config = None\n",
        "    self.preds = None\n",
        "    self.labels = None\n",
        "    self.eval_preds = None\n",
        "    self.logits = None\n",
        "    self.project_name = \"Fine_tuned_model_dir\"\n",
        "    # self.final_model = None  # Initialize final_model attribute here\n",
        "\n",
        "  # def postprocess_text(self, preds, labels):\n",
        "  #   preds = [pred.strip() for pred in preds]\n",
        "  #   labels = [label.strip() for label in labels]\n",
        "  #   # rougeLSum expects newline after each sentence\n",
        "  #   preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
        "  #   labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
        "  #   return  preds, labels\n",
        "\n",
        "  # def compute_metrics(self, eval_preds):\n",
        "  #   preds, labels = eval_preds\n",
        "  #   if isinstance(preds, tuple):\n",
        "  #       preds = preds[0]\n",
        "  #   decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "  #   # Replace -100 in the labels as we can't decode them.\n",
        "  #   labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)\n",
        "  #   decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "  #   # Some simple post-processing\n",
        "  #   decoded_preds, decoded_labels = self.postprocess_text(decoded_preds, decoded_labels)\n",
        "  #   self.result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "  #   self.result = {k: round(v * 100, 4) for k, v in self.result.items()}\n",
        "  #   prediction_lens = [np.count_nonzero(pred != self.tokenizer.pad_token_id) for pred in self.preds]\n",
        "  #   self.result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "  #   return self.result\n",
        "\n",
        "  # def preprocess_logits_for_metrics(self, logits, labels):\n",
        "  #   self.logits = logits\n",
        "  #   self.labels = labels\n",
        "  #   \"\"\"\n",
        "  #   Original Trainer may have a memory leak.\n",
        "  #   This is a workaround to avoid storing too many tensors that are not needed.\n",
        "  #   \"\"\"\n",
        "  #   self.pred_ids = torch.argmax(self.logits[0], dim=-1)\n",
        "  #   return self.pred_ids, self.labels\n",
        "\n",
        "\n",
        "  # def get_final_model(self):\n",
        "  #   self.final_model = self.final_model\n",
        "\n",
        "  def train(self):\n",
        "    project = self.project_name\n",
        "    base_model_name = self.model_name\n",
        "    run_name = base_model_name + \"-\" + project\n",
        "    self.output_dir = \"./\" + run_name\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model = self.final_model,\n",
        "        tokenizer = self.tokenizer,\n",
        "        train_dataset = self.train_data_tokenized, # will come from Dataset class\n",
        "        eval_dataset = self.eval_data_tokenized, # will come from Dataset class\n",
        "        # compute_metrics = self.compute_metrics,\n",
        "        # preprocess_logits_for_metrics= self.preprocess_logits_for_metrics,\n",
        "        args = TrainingArguments(\n",
        "            output_dir = self.output_dir,\n",
        "            warmup_steps = self.warmup_steps,\n",
        "            per_device_train_batch_size = self.per_device_train_batch_size,\n",
        "            per_device_eval_batch_size = self.per_device_eval_batch_size,\n",
        "            gradient_accumulation_steps = self.gradient_accumulation_steps,\n",
        "            gradient_checkpointing=True,\n",
        "            max_steps = self.max_steps,\n",
        "            learning_rate = self.learning_rate, # Want a small lr for finetuning\n",
        "            fp16 = self.fp16,\n",
        "            optim = self.optimizer,\n",
        "            logging_steps = self.logging_steps,              # When to start reporting loss\n",
        "            logging_dir = \"./logs\",        # Directory for storing logs\n",
        "            save_strategy = \"steps\",       # Save the model checkpoint every logging step\n",
        "            save_steps = self.save_steps,                # Save checkpoints every 50 steps\n",
        "            evaluation_strategy = \"steps\", # Evaluate the model every logging step\n",
        "            eval_steps = self.eval_steps,               # Evaluate and save checkpoints every 50 steps\n",
        "            do_eval = self.do_eval,                # Perform evaluation at the end of training\n",
        "            report_to = self.report_to,           # Comment this out if you don't want to use weights & baises\n",
        "            run_name = f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
        "        ),\n",
        "        data_collator = DataCollatorForLanguageModeling(self.tokenizer, mlm=False),\n",
        "    )\n",
        "    config = self.config\n",
        "\n",
        "    self.model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "    trainer.train()\n",
        "    trainer.save_model(f\"/content/T5-Finetunning-t5/checkpoint-{self.checkpoint_to_be_saved}\")\n",
        "    # self.final_model.save_pretrained(f\"/content/T5-Finetunning-t5/checkpoint-{sself.checkpoint_to_be_saved}\")\n",
        "    # self.tokenizer.save_pretrained(f\"/content/T5-Finetunning-t5/checkpoint-{self.checkpoint_to_be_saved}\")\n",
        "\n",
        "\n",
        "  def success_msg_train(self):\n",
        "    return \"Successfully Train model\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "iM_VYh96bLHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Inference(Model_train):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.ft_model = None\n",
        "    self.tokenizer = None\n",
        "    self.eval_prompt = None\n",
        "    self.eval_prompt = None\n",
        "    self.message = None\n",
        "    self.history = None\n",
        "    self.return_tensors=\"pt\"\n",
        "\n",
        "  def load_finetuned_model(self):\n",
        "    self.ft_model = PeftModel.from_pretrained(self.model, f\"/content/T5-Finetunning-t5/checkpoint-{50}\")  # self.checkpoint_to_be_saved\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, add_bos_token=True, trust_remote_code=True)\n",
        "\n",
        "  def response(self, eval_prompt):\n",
        "    print(eval_prompt)\n",
        "    self.model_input = self.tokenizer(eval_prompt, return_tensors=self.return_tensors).to(\"cuda\")  # Ensure correct return type\n",
        "    print(self.model_input)\n",
        "    # Convert potential lists to tensors (if needed)\n",
        "    if isinstance(self.model_input, list):\n",
        "      self.model_input = torch.tensor(self.model_input)\n",
        "    translation = self.ft_model.generate(**self.model_input)  # Use model.generate directly\n",
        "    if isinstance(translation, list):\n",
        "      translation = torch.tensor(translation)\n",
        "    translated_text = self.tokenizer.batch_decode(translation, max_new_tokens=512, skip_special_tokens=True)[0]\n",
        "\n",
        "  def success_msg_for_ft(self):\n",
        "    return \"Fine tuned model load successfully\"\n"
      ],
      "metadata": {
        "id": "8pl8n6foW9i5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# predict function for inference chatbot"
      ],
      "metadata": {
        "id": "ajJNCtd03dnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(message, history):\n",
        "    global inference\n",
        "    model_inputs = inference.tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\n",
        "    ## Streamer that stores print-ready text in a queue, to be used by a downstream application as an iterator.\n",
        "    ## This is useful for applications that benefit from acessing the generated text in a non-blocking way (e.g. in an interactive Gradio demo).\n",
        "    streamer = TextIteratorStreamer(inference.tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n",
        "    generate_kwargs = dict(\n",
        "        model_inputs,\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=True,\n",
        "        top_p=0.95,\n",
        "        top_k=1000,\n",
        "        temperature=1,\n",
        "        num_beams=1\n",
        "        )\n",
        "    t = Thread(target=inference.ft_model.generate, kwargs=generate_kwargs)\n",
        "    t.start()\n",
        "    history  = \"\"\n",
        "    for new_token in streamer:\n",
        "        if new_token != '<':\n",
        "            history += new_token\n",
        "            yield history"
      ],
      "metadata": {
        "id": "fo_LriCl2zYh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhC0pbD2NhdU"
      },
      "source": [
        "# Login Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "63MQcxvWKh-6"
      },
      "outputs": [],
      "source": [
        "class login_setup:\n",
        "  def HF_login(self, hf_token):\n",
        "    self.hf_token = hf_token\n",
        "    login(token = self.hf_token, add_to_git_credential=True)\n",
        "    return \"Successfully Login into HuggingFace\"\n",
        "\n",
        "  def wandb_login(self, wandb_token):\n",
        "    self.wandb_token = wandb_token\n",
        "    wandb.login(key = self.wandb_token)\n",
        "    return \"Successfully Login into Wandb\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Login\n"
      ],
      "metadata": {
        "id": "80DlpPaX93V_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOGIN = login_setup()"
      ],
      "metadata": {
        "id": "m7nm40pv9zq6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOGIN.HF_login(HUGGINGFACE_API_KEY)\n",
        "LOGIN.wandb_login(WANDB_API_KEY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ptSmfdVd9ywm",
        "outputId": "11aed25b-8278-4fa0-d01d-f1136761f73f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: read).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Successfully Login into Wandb'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wandb Class"
      ],
      "metadata": {
        "id": "WNuWQomD94PB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "M57ClHdequXg"
      },
      "outputs": [],
      "source": [
        "class wandb_visualize(Parameters):\n",
        "  def __init__(self, api = wandb.Api()):\n",
        "    self.api = api.project('huggingface').url\n",
        "\n",
        "  def wandb_report(self):\n",
        "    self.iframe = f'<iframe src={self.api} style=\"border:none;height:1024px;width:100%\">'\n",
        "    return gr.HTML(self.iframe)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nXqDvYigeCx"
      },
      "source": [
        "# Create Class instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "O5zcJjABqvwl"
      },
      "outputs": [],
      "source": [
        "model_train = Model_train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "U_9HXGQUqwaP"
      },
      "outputs": [],
      "source": [
        "WANDB = wandb_visualize()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference = Inference()"
      ],
      "metadata": {
        "id": "5TnrvNWabIEq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "SKMKYWgA4mZw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pravwI786-VE"
      },
      "outputs": [],
      "source": [
        "# model_train.set_config()\n",
        "# model_train.load_model()\n",
        "# model_train.load_tokenizer()\n",
        "# model_train.set_peft_config()\n",
        "# model_train.apply_accelerator_on_peft_model()\n",
        "# model_train.loading_dataset()\n",
        "# model_train.tokenized_data()\n",
        "# # Training\n",
        "# model_train.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW7EdnbHDEwe"
      },
      "source": [
        "# UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toOqkZLc7TXs"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks(gr.themes.Soft(primary_hue=gr.themes.colors.slate, secondary_hue=gr.themes.colors.purple)) as demo:\n",
        "\n",
        "  with gr.Tab(\"Parameters\"):\n",
        "    with gr.Row():\n",
        "      # UI for Dataset Parameters\n",
        "      with gr.Column():\n",
        "        dataset_name = gr.components.Textbox(label = \"Dataset Name\", info = \"Please provide a valid Dataset name from HuggingFace. i.e Amod/mental_health_counseling_conversations\", value = \"Amod/mental_health_counseling_conversations\")\n",
        "        data_split = gr.components.Textbox(label = \"HuggingFace Data Split\", value = \"train\")\n",
        "      with gr.Column():\n",
        "        question_column_name = gr.components.Textbox(label = \"First Column\", info = \"Please provide a valid column name from given dataset. i.e for this dataset Context.\", value = \"Context\")\n",
        "        answer_column_name = gr.components.Textbox(label = \"Second Column\", info = \"Please provide a valid column name from given dataset. i.e for this dataset Response.\", value = \"Response\")\n",
        "        train_size = gr.Slider(minimum = 0.6, maximum = 0.9, label = \"Train Size\", info = \"Define train size.\", value = 0.6, step = 0.01)\n",
        "    with gr.Row():\n",
        "      # UI for Config Parameters\n",
        "      with gr.Column():\n",
        "        r_value = gr.Slider(minimum = 4, maximum = 64, label = \"Rank\", info = \"Rank relates to the amount of trainable parameters LoRA will be able to use during training. i.e 4, 8, 16, 32.\", value = 32, step = 1)\n",
        "        lora_alpha = gr.Slider(minimum = 4, maximum = 64, label = \"Alpha\", info = \"Alpha is a scaling parameter. i.e 4, 8, 16, 32.\", value = 32, step = 1)\n",
        "        lora_dropout = gr.Slider(minimum = 0.01, maximum = 1.0, label = \"LoRA Dropout\", info = \" This parameter indicates the dropout probability for LoRA layers. In this configuration, the dropout rate is set to 0.05 or 5%.Range from 0.0 to 1.0\", value = 0.05, step = 0.01)\n",
        "\n",
        "\n",
        "    with gr.Row():\n",
        "      # UI for Model Parameters\n",
        "      model_name = gr.components.Textbox(label = \"Model name\", info = \"Please provide a valid model id from HuggingFace. i.e mistralai/Mistral-7B-v0.1\", value = \"mistralai/Mistral-7B-v0.1\")\n",
        "\n",
        "\n",
        "    with gr.Row():\n",
        "      # UI for Training Parameters\n",
        "      with gr.Column():\n",
        "        warmup_steps = gr.Slider(label = \"Warmup step \", info = \"Applying 50 warm-up steps means the learning rate will increase linearly from 0 to the initial learning rate set in the optimizer during the first 50 steps \", value = 1, step = 1)\n",
        "        per_device_train_batch_size = gr.Slider(minimum = 1, maximum = 512, label = \"Training Batch size\", info = \"Batch Size is the number of training examples used by one GPU in one training step.\", value = 4, step = 1)\n",
        "        per_device_eval_batch_size = gr.Slider(minimum = 1, maximum = 512, label = \"Evaluation Batch size\", info = \"Batch Size is the number of training examples used by one GPU in one training step.\", value = 4, step = 1)\n",
        "\n",
        "      with gr.Column():\n",
        "        gradient_accumulation_steps = gr.Slider(label = \"Gradient accumulation steps\", info = \"Gradient accumulation is a technique that simulates a larger batch size by accumulating gradients from multiple small batches before performing a weight update.\", value = 1)\n",
        "        max_steps = gr.Slider(minimum = 1, maximum = 100000, label = \"Max steps\", info = \"Number of steps you want to trained.\", value = 100, step = 1)\n",
        "        learning_rate = gr.Slider(minimum = 0.0, maximum =1.0, label = \"Learning Rate\", info = \"Learning Rate you want to use\", value = 2.5e-5, step = 0.0000001)\n",
        "\n",
        "      with gr.Column():\n",
        "        fp16 = gr.Checkbox(label = \"fp16\", info = \"Please stay check if you want to use this.\", value = True)\n",
        "        optimizer = gr.components.Textbox(label = \"Optimizer\", info = \"Please choose between [adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused, adamw_anyprecision, paged_adamw_8bit] or more if you want\", value = \"paged_adamw_8bit\")\n",
        "        logging_steps = gr.Slider(minimum = 1, maximum = 100000, label = \"Logging steps\", info = \"After every specified no. of steps you want to see logs.\", value = 50, step = 1)\n",
        "\n",
        "      with gr.Column():\n",
        "        save_steps = gr.Slider(minimum = 1, maximum = 100000, label = \"Save steps\", info = \"After every specified no. of steps you want to save checkpoints.\", value = 50, step = 1)\n",
        "        eval_steps = gr.Slider(minimum = 1, maximum = 100000, label = \"Evaluation steps\", info = \"After every specified no. of steps you want to evaluate.\", value = 50, step = 1)\n",
        "        report_to = gr.components.Textbox(label = \"Report to\", info = \"It only work on wandb.\", value = \"wandb\")\n",
        "        checkpoint_to_be_saved = gr.components.Textbox(label = \"Checkpoint saved\", info = \"Specify at which checkpoint you want to save your model. Default to max_step.\", value = 100)\n",
        "\n",
        "    with gr.Row():\n",
        "      # UI for Wandb Parameters\n",
        "      with gr.Column():\n",
        "        wandb_project_name = gr.components.Textbox(label = \"Wandb project name\", info = \"Specify your valid wandb project name.\", value = \"huggingface\")\n",
        "        success_message = gr.components.Textbox(label = \"Message\")\n",
        "\n",
        "    parameters_btn = gr.Button(\"Set Parameters\")\n",
        "    parameters_btn.click(model_train.set_dataset_name, inputs=[dataset_name]).then(model_train.set_data_split, inputs=[data_split]\n",
        "                        ).then(model_train.set_question_col_name, inputs=[question_column_name]).then(model_train.set_answer_col_name, inputs=[answer_column_name]\n",
        "                        ).then(model_train.set_r, inputs=[r_value]).then(model_train.set_lora_alpha, inputs=[lora_alpha]).then(model_train.set_lora_dropout, inputs=[lora_dropout]\n",
        "                        ).then(model_train.set_train_size, inputs=[train_size]).then(model_train.set_model_name, inputs=[model_name]).then(model_train.set_warmup_steps, inputs=[warmup_steps]\n",
        "                        ).then(model_train.set_per_device_train_batch_size, inputs=[per_device_train_batch_size]).then(model_train.set_per_device_eval_batch_size, inputs=[per_device_eval_batch_size]\n",
        "                        ).then(model_train.set_gradient_accumulation_steps, inputs=[gradient_accumulation_steps]).then(model_train.set_max_steps, inputs=[max_steps]\n",
        "                        ).then(model_train.set_learning_rate, inputs=[learning_rate]).then(model_train.set_fp16, inputs=[fp16]\n",
        "                        ).then(model_train.set_optimizer, inputs=[optimizer]).then(model_train.set_logging_steps, inputs=[logging_steps]\n",
        "                        ).then(model_train.set_logging_steps, inputs=[logging_steps]).then(model_train.set_save_steps, inputs=[save_steps]\n",
        "                        ).then(model_train.set_eval_steps, inputs=[eval_steps]).then(model_train.set_report_to, inputs=[report_to]\n",
        "                        ).then(model_train.set_checkpoint_to_be_saved, inputs=[checkpoint_to_be_saved]).then(model_train.set_wandb_project_name, inputs=[wandb_project_name]\n",
        "                        ).then(model_train.success_msg_params, outputs = success_message)\n",
        "\n",
        "  with gr.Tab(\"Data and Model\"):\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            model_and_tokenizer_load = gr.components.Textbox(label = \"Message\")\n",
        "\n",
        "    model_and_tokenizer_load_btn = gr.Button(\"load model and tokenizer\")\n",
        "    model_and_tokenizer_load_btn.click(model_train.set_config).then(model_train.load_model).then(model_train.load_tokenizer\n",
        "                                      ).then(model_train.set_peft_config).then(model_train.apply_accelerator_on_peft_model\n",
        "                                      ).then(model_train.success_msg_load, outputs=model_and_tokenizer_load, show_progress=True)\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            data_success_msg = gr.components.Textbox(label = \"Message\")\n",
        "\n",
        "    data_setup_btn = gr.Button(\"Data\")\n",
        "    data_setup_btn.click(model_train.tokenized_data).then(model_train.success_msg_data, outputs=data_success_msg, show_progress=True)\n",
        "\n",
        "  with gr.Tab(\"Train\"):\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        train_btn = gr.Button(\"Train\")\n",
        "        train_btn.click(model_train.train, show_progress=True)\n",
        "        report = WANDB.wandb_report()\n",
        "\n",
        "  with gr.Tab(\"Inference\"):\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        load_fine_tuned_model = gr.components.Textbox(label = \"Message\", info = \"Please click this button to load fine tuned model that\")\n",
        "        load_fine_tuned_model_btn = gr.Button(\"Load Fine tune model\")\n",
        "        load_fine_tuned_model_btn.click(inference.set_config, show_progress=True).then(inference.load_model, show_progress=True\n",
        "                                       ).then(inference.load_finetuned_model, show_progress=True).then(inference.success_msg_for_ft, outputs=[load_fine_tuned_model], show_progress=True)\n",
        "\n",
        "  # with gr.Tab(\"Chat\"):\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        chat = gr.ChatInterface(\n",
        "        predict,\n",
        "        chatbot=gr.Chatbot(render=False, height=600)\n",
        "        )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  demo.launch(share=True, debug=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEwEaC_ik_C0"
      },
      "source": [
        "https://medium.com/@fartypantsham/what-rank-r-and-alpha-to-use-in-lora-in-llm-1b4f025fd133"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "SYamZRYg9bIg",
        "IqUJE8Qc3mkr",
        "iP7GJQ-fJSwB",
        "nYg9-7fIJVqR",
        "M844CZZchPrA",
        "NhC0pbD2NhdU",
        "80DlpPaX93V_",
        "WNuWQomD94PB",
        "1nXqDvYigeCx",
        "SKMKYWgA4mZw"
      ],
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}