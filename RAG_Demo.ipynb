{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPmgA12nZ2aTBJlPB66n8sn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f351dd76947d497ba7433e6c99618e78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b35e42a41b5a42838c5df3d2fda330ba",
              "IPY_MODEL_51de09d475da4d07a5846a5df9938de9",
              "IPY_MODEL_a3369002b2d842299c3267abee89025e"
            ],
            "layout": "IPY_MODEL_229556ae84d1422690b150ead763a5c8"
          }
        },
        "b35e42a41b5a42838c5df3d2fda330ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66c687af2e0341ea9d327b560483e93a",
            "placeholder": "​",
            "style": "IPY_MODEL_290ca86fe106466fa1f47674c73c5de6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "51de09d475da4d07a5846a5df9938de9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d63dc05968594771881e6bef78adcb69",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92bdb1baf3ef4b1d8f53fb6f29face04",
            "value": 2
          }
        },
        "a3369002b2d842299c3267abee89025e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76ea85eaa11846aabd258f7ca800efa4",
            "placeholder": "​",
            "style": "IPY_MODEL_4fa3b2a184bd4a00bcb75d03621b54aa",
            "value": " 2/2 [00:09&lt;00:00,  4.57s/it]"
          }
        },
        "229556ae84d1422690b150ead763a5c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66c687af2e0341ea9d327b560483e93a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "290ca86fe106466fa1f47674c73c5de6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d63dc05968594771881e6bef78adcb69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92bdb1baf3ef4b1d8f53fb6f29face04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76ea85eaa11846aabd258f7ca800efa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fa3b2a184bd4a00bcb75d03621b54aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/highplainscomputing/Mistral-Gradio-fine-tuning/blob/main/RAG_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "voZv9isaAExl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c3oJyVq0LcRx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77437432-d5e6-46af-99e6-76d2f54e22ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.2/308.2 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4\n",
        "!pip install -q sentence-transformers\n",
        "# !pip install -q faiss-cpu\n",
        "!pip install -q torch datasets\n",
        "!pip install -q pypdf\n",
        "!pip install -q tqdm\n",
        "!pip install -q faiss-gpu\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q gradio\n",
        "!pip install -q trl==0.4.7\n",
        "!pip install -q ipywidgets==7.7.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dependencies"
      ],
      "metadata": {
        "id": "km2ornvJ9ft5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
        "from langchain_community.document_loaders import BSHTMLLoader\n",
        "from io import StringIO\n",
        "from datasets import load_dataset, Dataset\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain import hub\n",
        "from langchain import HuggingFaceHub\n",
        "import gradio as gr\n",
        "import os\n",
        "import shutil\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "YjxabIdYLdN0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Variables"
      ],
      "metadata": {
        "id": "4XtOACw0PSD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOAD_IN_4BIT = True\n",
        "BNB_4BIT_USE_DOUBLE_QUANT = True\n",
        "BNB_4BIT_QUANT_TYPE = \"nf4\"\n",
        "ADD_EOS_TOKEN = True\n",
        "PADDING_SIDE = \"left\"\n",
        "ADD_BOS_TOKEN = True\n",
        "HUGGINGFACE_API_KEY = \"Your_Huggingface_API_KEY\"\n",
        "\n",
        "os.mkdir(\"data\")"
      ],
      "metadata": {
        "id": "CxNpWqqiQRUP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters"
      ],
      "metadata": {
        "id": "GlyEzEOWPQJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAG_Parameters:\n",
        "  def __init__(self, files = None, question_col_name = \"Context\", answer_col_name = \"Response\",model_name = \"mistralai/Mistral-7B-v0.1\",\n",
        "               prompt = \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\",\n",
        "               embedding_model = \"sentence-transformers/all-mpnet-base-v2\", temperature = 0.7, max_new_tokens = 128, repetition_penalty = 1.15,\n",
        "               top_k = 1, top_p = 0.75, k_context = 4, chunk_size = 512, chunk_overlap = 100):\n",
        "\n",
        "    # Dataset Parameters\n",
        "    self.files = files\n",
        "    self.question_col_name = question_col_name\n",
        "    self.answer_col_name = answer_col_name\n",
        "\n",
        "    # Model Parameters\n",
        "    self.model_name = model_name\n",
        "\n",
        "    # Config Parameters\n",
        "    self.prompt = prompt\n",
        "    self.embedding_model = embedding_model\n",
        "    self.temperature = temperature\n",
        "    self.max_new_tokens = max_new_tokens\n",
        "    self.repetition_penalty = repetition_penalty\n",
        "    self.top_k = top_k\n",
        "    self.top_p = top_p\n",
        "    self.k_context = k_context\n",
        "    self.chunk_size = chunk_size\n",
        "    self.chunk_overlap = chunk_overlap\n",
        "\n",
        "  # Functions to modify variables\n",
        "  def set_files(self, files):\n",
        "      self.files = files\n",
        "\n",
        "  def set_question_col_name(self, question_col_name):\n",
        "      self.question_col_name = question_col_name\n",
        "\n",
        "  def set_answer_col_name(self, answer_col_name):\n",
        "      self.answer_col_name = answer_col_name\n",
        "\n",
        "  def set_model_name(self, model_name):\n",
        "      self.model_name = model_name\n",
        "\n",
        "  def set_prompt(self, prompt):\n",
        "    self.prompt = prompt\n",
        "\n",
        "  def set_embedding_model(self, embedding_model):\n",
        "      self.embedding_model = embedding_model\n",
        "\n",
        "  def set_temperature(self, temperature):\n",
        "      self.temperature = temperature\n",
        "\n",
        "  def set_max_new_tokens(self, max_new_tokens):\n",
        "    self.max_new_tokens = max_new_tokens\n",
        "\n",
        "  def set_repetition_penalty(self, repetition_penalty):\n",
        "      self.repetition_penalty = repetition_penalty\n",
        "\n",
        "  def set_top_k(self, top_k):\n",
        "      self.top_k = top_k\n",
        "\n",
        "  def set_top_p(self, top_p):\n",
        "    self.top_p = top_p\n",
        "\n",
        "  def set_k_context(self, k_context):\n",
        "    self.k_context = k_context\n",
        "\n",
        "  def set_chunk_size(self, chunk_size):\n",
        "    self.chunk_size = chunk_size\n",
        "\n",
        "  def set_chunk_overlap(self, chunk_overlap):\n",
        "    self.chunk_overlap = chunk_overlap\n",
        "\n",
        "  def success_msg_params(self):\n",
        "    return \"Parameters set successfully\"\n",
        "\n"
      ],
      "metadata": {
        "id": "uje-qnkKLpnn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "kpeCu2LXPORA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(RAG_Parameters):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.config = None\n",
        "\n",
        "\n",
        "  def set_config(self):\n",
        "    self.bnb_config = BitsAndBytesConfig(\n",
        "                                        load_in_4bit = LOAD_IN_4BIT,\n",
        "                                        bnb_4bit_use_double_quant = BNB_4BIT_USE_DOUBLE_QUANT,\n",
        "                                        bnb_4bit_quant_type = BNB_4BIT_QUANT_TYPE,\n",
        "                                        bnb_4bit_compute_dtype = torch.bfloat16\n",
        ")\n",
        "  def load_model(self):\n",
        "    self.set_config()\n",
        "    self.model = AutoModelForCausalLM.from_pretrained(self.model_name, quantization_config = self.bnb_config, low_cpu_mem_usage=True)\n",
        "\n",
        "  def load_tokenizer(self):\n",
        "   tokenizer = AutoTokenizer.from_pretrained(\n",
        "                                            self.model_name,\n",
        "                                            padding_side=PADDING_SIDE,\n",
        "                                            add_eos_token=ADD_EOS_TOKEN,\n",
        "                                            add_bos_token=ADD_BOS_TOKEN,\n",
        "                                            )\n",
        "   tokenizer.pad_token = tokenizer.eos_token\n",
        "   self.tokenizer = tokenizer\n",
        "  #  return self.tokenizer\n",
        "\n",
        "  def success_msg_load(self):\n",
        "    return \"Successfully load Model and Tokenizer\""
      ],
      "metadata": {
        "id": "jRwCJHWDLpj1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "n0d_4VABPM9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Data(Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.directory = \"/content/data\"\n",
        "\n",
        "  def loading_dataset(self):\n",
        "    print(f\"Files duplicates : {self.files}\")\n",
        "    self.files = list(set(self.files)) # remove duplicate path\n",
        "    print(f\"Files without duplicates : {self.files}\")\n",
        "    self.file_extension = self.files[0]\n",
        "    # print(f\"Files : {self.files}\")\n",
        "    # print(\"-------------------------------------------------------------------------------------------------------------\")\n",
        "    # print(f\"Type of files {type(self.files)}\")\n",
        "    for file_path in self.files:\n",
        "    # Extract the file name from the file path\n",
        "      file_name = os.path.basename(file_path)\n",
        "      # Construct the destination path\n",
        "      destination_path = os.path.join(self.directory, file_name)\n",
        "      # Move the file\n",
        "      shutil.move(file_path, destination_path)\n",
        "      print(f\"Moved {file_name} to {self.directory}\")\n",
        "\n",
        "  def get_file_types(self):\n",
        "    file_types = set()\n",
        "    for filename in os.listdir(self.directory):\n",
        "      if os.path.isfile(os.path.join(self.directory, filename)):\n",
        "        file_extension = filename.split(\".\")[-1].lower()\n",
        "        file_types.add(file_extension)\n",
        "\n",
        "      self.file_types_string = \", \".join(file_types)\n",
        "      self.number_of_files = len(os.listdir(self.directory))\n",
        "\n",
        "      return self.file_types_string, self.number_of_files\n",
        "\n",
        "\n",
        "  def create_vector_store_index(self):\n",
        "    file_extension, directory = self.get_file_types()\n",
        "\n",
        "    if file_extension == \"md\":\n",
        "      loader = DirectoryLoader(\n",
        "          self.directory,\n",
        "          glob=\"*.md\",\n",
        "          loader_cls=TextLoader,\n",
        "          show_progress=True,\n",
        "          )\n",
        "      pages = loader.load()\n",
        "      text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = self.chunk_size,\n",
        "      chunk_overlap = self.chunk_overlap,\n",
        "      )\n",
        "      self.documents = text_splitter.split_documents(pages)\n",
        "\n",
        "    elif file_extension == \"pdf\":\n",
        "      loader = DirectoryLoader(\n",
        "      self.directory,\n",
        "      glob=\"*.pdf\",\n",
        "      loader_cls=PyPDFLoader,\n",
        "      show_progress=True,\n",
        "      )\n",
        "      pages = loader.load()\n",
        "      text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = self.chunk_size,\n",
        "      chunk_overlap = self.chunk_overlap,\n",
        "      )\n",
        "      self.documents = text_splitter.split_documents(pages)\n",
        "    elif file_extension == \"txt\":\n",
        "      loader = DirectoryLoader(\n",
        "      self.directory,\n",
        "      glob=\"*.txt\",\n",
        "      loader_cls=TextLoader,\n",
        "      show_progress=True,\n",
        "      )\n",
        "      pages = loader.load()\n",
        "      text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = self.chunk_size,\n",
        "      chunk_overlap = self.chunk_overlap,\n",
        "      )\n",
        "      self.documents = text_splitter.split_documents(pages)\n",
        "    return self.documents\n",
        "\n",
        "\n",
        "  def success_msg_data(self):\n",
        "    return \"Documents created successfully\"\n"
      ],
      "metadata": {
        "id": "HQ1YwOOcLpgT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Database"
      ],
      "metadata": {
        "id": "B5Q8QvgQ9lwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Create_vector(Data):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def load_embedding_model(self):\n",
        "      self.embeddings = HuggingFaceEmbeddings(model_name=self.embedding_model)\n",
        "\n",
        "  def create_db(self):\n",
        "      self.vectordb = FAISS.from_documents(self.documents, self.embeddings)\n",
        "      vector = self.vectordb\n",
        "      return vector\n",
        "  def success_msg_data_db(self):\n",
        "    return \"Embedding Model and Vector Database created successfully\""
      ],
      "metadata": {
        "id": "vUK9a4rRwICs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline and LLM"
      ],
      "metadata": {
        "id": "qGetLThQ59hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Pipeline_and_llm(Create_vector):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.nothing = None\n",
        "\n",
        "  def create_pipeline(self):\n",
        "    pipe = pipeline(\n",
        "        model=self.model,\n",
        "        task='text-generation',\n",
        "        tokenizer=self.tokenizer,\n",
        "        temperature=self.temperature,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "        max_new_tokens=self.max_new_tokens,  # mex number of tokens to generate in the output\n",
        "        repetition_penalty=self.repetition_penalty,  # without this output begins repeating\n",
        "        top_k=self.top_k,\n",
        "        top_p=self.top_p,\n",
        "    )\n",
        "    self.pipeline = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "    self.retriever = self.vectordb.as_retriever(search_kwargs={\"k\": self.k_context}) # set here to that all parameters can be set in UI\n",
        "\n",
        "    template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
        "    {context}\n",
        "    Question: {question}\n",
        "    Helpful Answer:\"\"\"\n",
        "    self.prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "  def llm_chain(self):\n",
        "    self.chain = LLMChain(llm=self.pipeline, prompt=self.prompt)\n",
        "\n",
        "\n",
        "  def retriever_chain(self):\n",
        "    self.retrieval_chain = (\n",
        "      {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
        "      | self.chain\n",
        "    )\n",
        "\n",
        "  def success_msg_pipeline(self):\n",
        "    return \"Successfully created Pipeline and LLM chain\"\n",
        "\n",
        "  def inference(self, query):\n",
        "    self.query = query\n",
        "\n",
        "    if self.query is None:\n",
        "        raise ValueError(\"Query is not set. Please provide a query.\")\n",
        "\n",
        "    answer = self.retrieval_chain.invoke(self.query)\n",
        "    context = answer.get(\"context\")\n",
        "    context_processed = [context[i].page_content.replace(\"\\n\", \" \") for i in range(len(context))]\n",
        "    meta_data = [context[i].metadata for i in range(len(context))]\n",
        "    # post_processed_answer = answer[\"text\"].split(\"Helpful Answer:\")[-1].strip()\n",
        "    return context_processed, meta_data, answer.get(\"text\")"
      ],
      "metadata": {
        "id": "U8cZH1g7whVi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instance"
      ],
      "metadata": {
        "id": "tdijcG1t6NqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Chatbot = Pipeline_and_llm()"
      ],
      "metadata": {
        "id": "kxG8QnpUx6kK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Login"
      ],
      "metadata": {
        "id": "RKJcATIDPU2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class login_setup:\n",
        "  def HF_login(self, hf_token):\n",
        "    self.hf_token = hf_token\n",
        "    login(token = self.hf_token, add_to_git_credential=True)\n",
        "    return \"Successfully Login into HuggingFace\""
      ],
      "metadata": {
        "id": "d35qmWHAQzuE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOGIN = login_setup()\n",
        "LOGIN.HF_login(HUGGINGFACE_API_KEY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "-wQ3Th9UU-2U",
        "outputId": "8745e52b-8b9c-42ff-8ae3-0f5da3c6e7d9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: read).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Successfully Login into HuggingFace'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UI"
      ],
      "metadata": {
        "id": "JexreQeLRfi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(theme = \"ParityError/anime\") as demo:\n",
        "  with gr.Tab(\"Parameters\"):\n",
        "\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        gr.Markdown(\"File must be PDF\")\n",
        "        files_upload = gr.File(type='filepath', file_count=\"directory\")\n",
        "      with gr.Column():\n",
        "        question_column_name = gr.components.Textbox(label = \"First Column\", info = \"Please provide a valid column name from given dataset. i.e for this dataset Context.\", value = \"Questions\")\n",
        "        answer_column_name = gr.components.Textbox(label = \"Second Column\", info = \"Please provide a valid column name from given dataset. i.e for this dataset Response.\", value = \"Answers\")\n",
        "\n",
        "    with gr.Row():\n",
        "      # UI for Model Parameters\n",
        "      model_name = gr.components.Textbox(label = \"Model name\", info = \"Please provide a valid model id from HuggingFace. i.e mistralai/Mistral-7B-v0.1\", value = \"mistralai/Mistral-7B-v0.1\")\n",
        "\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        prompt = gr.components.Textbox(label = \"Prompt\", info = \"Please provide your own prompt template if you want.\", value = \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\")\n",
        "\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        embedding_model = gr.components.Textbox(label = \"Embedding Model\", info = \"Please provide a embedding model from Huggingface.\", value = \"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        temperature = gr.Slider(minimum = 0, maximum = 1, label = \"Temperature\", info = \" Close to 1 general, close to zero specific to data.\", value = 0.7, step = 0.1)\n",
        "        max_new_tokens = gr.Slider(minimum = 100, maximum = 1024, label = \"Max New Tokens\", info = \"Number of max new token you wan.\", value = 100, step = 1)\n",
        "        repetition_penalty = gr.Slider(minimum = 0.1, maximum = 100, label = \"Repetition Penalty\", info = \"\", value = 1.15, step = 0.01)\n",
        "\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        top_k = gr.Slider(minimum = 0.1, maximum = 100, label = \"Top k\", info = \"\", value = 1.15, step = 0.01)\n",
        "        top_p = gr.Slider(minimum = 0.1, maximum = 100, label = \"Top p\", info = \"\", value = 1.15, step = 0.01)\n",
        "        k_context = gr.Slider(minimum = 1, maximum = 10, label = \"K context\", info = \"\", value = 3, step = 1)\n",
        "\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        chunk_size = gr.Slider(minimum = 100, maximum = 2048, label = \"Chunk Size\", info = \"\", value = 1024, step = 1)\n",
        "        chunk_overlap = gr.Slider(minimum = 100, maximum = 2048, label = \"Chunk Overlap\", info = \"\", value = 100, step = 1)\n",
        "\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        success_message = gr.components.Textbox(label = \"Message\")\n",
        "\n",
        "    parameters_btn = gr.Button(\"Set Parameters\")\n",
        "    parameters_btn.click(Chatbot.set_files, inputs=[files_upload]).then(Chatbot.set_question_col_name, inputs=[question_column_name]).then(Chatbot.set_answer_col_name, inputs=[answer_column_name]\n",
        "                        ).then(Chatbot.set_prompt, inputs=[prompt]).then(Chatbot.set_embedding_model, inputs = [embedding_model]).then(Chatbot.set_temperature, inputs = [temperature]\n",
        "                        ).then(Chatbot.set_max_new_tokens, inputs = [max_new_tokens]).then(Chatbot.set_repetition_penalty, inputs = [repetition_penalty]\n",
        "                        ).then(Chatbot.set_top_k, inputs = [top_k]).then(Chatbot.set_chunk_size, inputs = [chunk_size]).then(Chatbot.set_chunk_overlap, inputs = [chunk_overlap]\n",
        "                        ).then(Chatbot.success_msg_params, outputs=[success_message])\n",
        "\n",
        "  with gr.Tab(\"Data, Model and DataBase\"):\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            model_and_tokenizer_load = gr.components.Textbox(label = \"Message\")\n",
        "\n",
        "    model_and_tokenizer_load_btn = gr.Button(\"Load Model and Tokenizer\")\n",
        "    model_and_tokenizer_load_btn.click(Chatbot.set_config).then(Chatbot.load_model).then(Chatbot.load_tokenizer\n",
        "                                      ).then(Chatbot.success_msg_load, outputs=model_and_tokenizer_load, show_progress=True)\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            data_success_msg_data = gr.components.Textbox(label = \"Message\")\n",
        "    data_setup_btn = gr.Button(\"Documents\")\n",
        "    data_setup_btn.click(Chatbot.loading_dataset).then(Chatbot.create_vector_store_index).then(Chatbot.success_msg_data, outputs=data_success_msg_data, show_progress=True)\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            data_success_msg_db = gr.components.Textbox(label = \"Message\")\n",
        "            vector_display = gr.components.Textbox(label = \"Message\", lines = 8)\n",
        "    data_setup_btn = gr.Button(\"Database\")\n",
        "    data_setup_btn.click(Chatbot.load_embedding_model).then(Chatbot.create_db, outputs = vector_display, show_progress=True).then(Chatbot.success_msg_data_db, outputs=data_success_msg_db, show_progress=True)     # (Chatbot.create_vector_store_index).then\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            data_success_msg_pipeline = gr.components.Textbox(label = \"Message\")\n",
        "    data_setup_btn = gr.Button(\"Pipeline\")\n",
        "    data_setup_btn.click(Chatbot.create_pipeline).then(Chatbot.llm_chain).then(Chatbot.retriever_chain).then(Chatbot.success_msg_pipeline, outputs=data_success_msg_pipeline, show_progress=True)\n",
        "\n",
        "  with gr.Tab(\"Inference\"):\n",
        "      with gr.Row():\n",
        "          with gr.Column():\n",
        "              inference_input = gr.components.Textbox(label = \"Submit\", info = \"Write query related to your docs.\")\n",
        "              inference_output_question = gr.components.Textbox(label = \"Relevant Text\", lines=5)\n",
        "              inference_output_context = gr.components.Textbox(label = \"Metadata\", lines=3)\n",
        "              inference_output_text = gr.components.Textbox(label = \"Answer\")\n",
        "\n",
        "      btn = gr.Button(\"Generate\")\n",
        "      btn.click(Chatbot.inference, inputs=[inference_input], outputs=[inference_output_question, inference_output_context, inference_output_text], show_progress=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  demo.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "id": "Erp1SKcALooh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782,
          "referenced_widgets": [
            "f351dd76947d497ba7433e6c99618e78",
            "b35e42a41b5a42838c5df3d2fda330ba",
            "51de09d475da4d07a5846a5df9938de9",
            "a3369002b2d842299c3267abee89025e",
            "229556ae84d1422690b150ead763a5c8",
            "66c687af2e0341ea9d327b560483e93a",
            "290ca86fe106466fa1f47674c73c5de6",
            "d63dc05968594771881e6bef78adcb69",
            "92bdb1baf3ef4b1d8f53fb6f29face04",
            "76ea85eaa11846aabd258f7ca800efa4",
            "4fa3b2a184bd4a00bcb75d03621b54aa"
          ]
        },
        "outputId": "5934645a-f9d1-421f-9dcd-ca5fcf75cb17"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://463da40eb0df97e1cc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://463da40eb0df97e1cc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f351dd76947d497ba7433e6c99618e78"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files duplicates : ['/tmp/gradio/78235aaf28039bbe83f2f10d6660ecb991c88eb9/1506.02640.pdf', '/tmp/gradio/4ba72887b96b1caf8086f203185c5bf940fd63f1/1706.03762.pdf']\n",
            "Files without duplicates : ['/tmp/gradio/78235aaf28039bbe83f2f10d6660ecb991c88eb9/1506.02640.pdf', '/tmp/gradio/4ba72887b96b1caf8086f203185c5bf940fd63f1/1706.03762.pdf']\n",
            "Moved 1506.02640.pdf to /content/data\n",
            "Moved 1706.03762.pdf to /content/data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:01<00:00,  1.40it/s]\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://463da40eb0df97e1cc.gradio.live\n"
          ]
        }
      ]
    }
  ]
}